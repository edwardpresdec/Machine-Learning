% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Data Mining Final Project},
  pdfauthor={by Edward Prescott-Decie},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Data Mining Final Project}
\author{by Edward Prescott-Decie}
\date{(December 02, 2024)}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\section{Introduction}\label{introduction}

This project is concerned with a dataset concerning forest fires in
regions of Canada. The aim of the project is to develop machine learning
models to predict the occurrence of these forest fires based on a
variety of environmental features such as temperature, humidity, wind
speed etc. A preprocessing and an exploratory analysis of the dataset
are conducted to obtain a better idea of the nature of the data.
Additionally, feature selection methods are explored in order to gain
insight on the significance of predictors in the set. Next, different
predictive techniques including, logistic regression, linear
discriminant analysis (LDA), quadratic discriminant analysis (QDA),
k-nearest neighbors (KNN), decision trees, random forests, boosting
methods, and support vector machines (SVM) are used to explore the
dataset and predict occurrences. Methods of optimizing these models are
also explored. Cross validation and hyperparameter tuning are used to
ensure optimal predictive efficacy. Many of the models in this project
aid in the inference of the data and the environment. It is important to
not only provide strong predictions but also solid understandings of the
data interactions that lead to these predictions. For example, if
scientists learn that one predictor is much more significant than
others, they can then afford to narrow their focus towards it. The goal
of the project is therefore, to not only explore how different models
perform, tune their hyperparameters, and assess their effectiveness in
predicting forest fires, but also gain inference and understanding of
the nature and interactions of elements within the data.

\section{Libraries}\label{libraries}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(cowplot)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(leaps)}
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{library}\NormalTok{(FactoMineR)}
\FunctionTok{library}\NormalTok{(pROC)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(class)}
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(rpart.plot)}
\FunctionTok{library}\NormalTok{(randomForest)}
\FunctionTok{library}\NormalTok{(gbm)}
\FunctionTok{library}\NormalTok{(adabag)}
\FunctionTok{library}\NormalTok{(e1071)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Tidyverse is mainly used to facilitate data handling and manipulation.
  It is also important for plotting data as it contains a sublibrary
  (ggplot2) which offers neater and more visually appealing outputs.
\item
  Cowplot is used to create subplots from plots generated with ggplot2.
\item
  Car is used for VIF calculation.
\item
  Caret is used for model training as well as generating more advanced
  confusion matrices.
\item
  Leaps is used for subset and feature selection.
\item
  Factoextra provides all the relevant functions to visualize the
  outputs of the principal component analysis. These functions include
  scree plot, biplot and Cos2.
\item
  PROC is used to generate ROC curves and calculate AUC. Aditionally,
  this package is useful in creating multiple ROC curves.
\item
  MASS is used for lda and qda modelling.
\item
  Class is used for KNN modelling.
\item
  Rpart is used in the making of decision trees and rpart.plot for the
  plotting of these trees
\item
  RandomForest will be used to implement functions related to tree
  classifiers such as bagging and boosting.
\item
  Gbm is used for bagging and adabag for boosting.
\item
  e1071 is used for functions related to support vector machines (SVM)
\end{itemize}

\section{Data Preprocessing}\label{data-preprocessing}

\subsection{Loading the Dataset}\label{loading-the-dataset}

To get started working with the data, the dataset must first be loaded:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ForestFires}\OtherTok{=}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"forest\_fires\_dataset.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{na.strings=}\StringTok{"?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Initial Viewing and
Preprocessing}\label{initial-viewing-and-preprocessing}

The original dimensions of the data are as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(ForestFires)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 247  14
\end{verbatim}

However, there is a split in the dataset between the data taken from
Cordillera and from Hudson Bay. In order to differentiate between this
data and later combine the two together, separate dataframes must be
created. By observing the data on the csv file, the indices to make the
split are chosen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fires\_cordillera }\OtherTok{=}\NormalTok{ ForestFires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{122}\NormalTok{,]}
\NormalTok{fires\_newhudson }\OtherTok{=}\NormalTok{ ForestFires[}\DecValTok{126}\SpecialCharTok{:}\DecValTok{247}\NormalTok{,]}
\FunctionTok{dim}\NormalTok{(fires\_cordillera)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 122  14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(fires\_newhudson)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 122  14
\end{verbatim}

The two new dataframes are of equal dimension. Additionally, they begin
and end on the same date simplifying a comparison-based analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(fires\_cordillera)}
\FunctionTok{head}\NormalTok{(fires\_newhudson)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   day month year Temperature RH Ws Rain FFMC DMC   DC ISI BUI FWI     Classes
## 1   1     6 2012          29 57 18    0 65.7 3.4  7.6 1.3 3.4 0.5 not fire   
## 2   2     6 2012          29 61 13  1.3 64.4 4.1  7.6   1 3.9 0.4 not fire   
## 3   3     6 2012          26 82 22 13.1 47.1 2.5  7.1 0.3 2.7 0.1 not fire   
## 4   4     6 2012          25 89 13  2.5 28.6 1.3  6.9   0 1.7   0 not fire   
## 5   5     6 2012          27 77 16    0 64.8   3 14.2 1.2 3.9 0.5 not fire   
## 6   6     6 2012          31 67 14    0 82.6 5.8 22.2 3.1   7 2.5     fire   
##     day month year Temperature RH Ws Rain FFMC DMC   DC ISI BUI FWI     Classes
## 126   1     6 2012          32 71 12  0.7 57.1 2.5  8.2 0.6 2.8 0.2 not fire   
## 127   2     6 2012          30 73 13    4 55.7 2.7  7.8 0.6 2.9 0.2 not fire   
## 128   3     6 2012          29 80 14    2 48.7 2.2  7.6 0.3 2.6 0.1 not fire   
## 129   4     6 2012          30 64 14    0 79.4 5.2 15.4 2.2 5.6   1 not fire   
## 130   5     6 2012          32 60 14  0.2 77.1   6 17.6 1.8 6.5 0.9 not fire   
## 131   6     6 2012          35 54 11  0.1 83.7 8.4 26.3 3.1 9.3 3.1     fire
\end{verbatim}

The day, month and year columns are then combined into a single column
in order to more easily work with this entry. Working with any of the
three alone, there would be many repeat values. Therefore, in order to
treat the data as a time series, they are combined it into a single
entry.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fires\_newhudson}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{=} \FunctionTok{paste}\NormalTok{(fires\_newhudson}\SpecialCharTok{$}\NormalTok{year,fires\_newhudson}\SpecialCharTok{$}\NormalTok{month,fires\_newhudson}\SpecialCharTok{$}\NormalTok{day,}\AttributeTok{sep=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fires\_cordillera}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{=} \FunctionTok{paste}\NormalTok{(fires\_cordillera}\SpecialCharTok{$}\NormalTok{year,fires\_cordillera}\SpecialCharTok{$}\NormalTok{month,fires\_cordillera}\SpecialCharTok{$}\NormalTok{day,}\AttributeTok{sep=}\StringTok{"{-}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The day, month and year columns are then removed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fires\_newhudson }\OtherTok{=}\NormalTok{ fires\_newhudson[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{15}\NormalTok{]}
\NormalTok{fires\_cordillera }\OtherTok{=}\NormalTok{ fires\_cordillera[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{15}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Additionally, it would be useful to insert a column into each dataframe
that contains information about the region which the entry came from.
This way, when combining the two dataframes, there will still be a
relevant differentiation. The two new columns at the end of the
dataframe can now be viewed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(fires\_newhudson)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Temperature RH Ws Rain FFMC DMC   DC ISI BUI FWI     Classes     Date
## 126          32 71 12  0.7 57.1 2.5  8.2 0.6 2.8 0.2 not fire    2012-6-1
## 127          30 73 13    4 55.7 2.7  7.8 0.6 2.9 0.2 not fire    2012-6-2
## 128          29 80 14    2 48.7 2.2  7.6 0.3 2.6 0.1 not fire    2012-6-3
## 129          30 64 14    0 79.4 5.2 15.4 2.2 5.6   1 not fire    2012-6-4
## 130          32 60 14  0.2 77.1   6 17.6 1.8 6.5 0.9 not fire    2012-6-5
## 131          35 54 11  0.1 83.7 8.4 26.3 3.1 9.3 3.1     fire    2012-6-6
\end{verbatim}

A combined dataframe housing both sets of data is then created. We note
that the dimension is now 244 rows rather than 247. We have simply
gotten rid of the blank space between both sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined\_fires }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(fires\_cordillera,fires\_newhudson)}
\FunctionTok{dim}\NormalTok{(combined\_fires)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 244  12
\end{verbatim}

\subsection{Class Balance/Imbalance + Missing
Data}\label{class-balanceimbalance-missing-data}

Another useful piece of information would be to gather data about the
number of entries belonging to each class. A bar graph can be plotted to
get an idea of the balance between the two sets. A relatively equal
number of entries in both classes is optimal as an imbalance could skew
or provide suboptimal results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes,}\AttributeTok{fill=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-10-1.pdf}

However, when making this bar plot, two classes are expected. However,
the bar plot shows 9. This can most likely be attributed to a formatting
problem and therefore the white space around the entries should be
trimmed. The plot is then attempted again and this seems to correct this
error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined\_fires}\SpecialCharTok{$}\NormalTok{Classes }\OtherTok{=} \FunctionTok{trimws}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Classes)}
\FunctionTok{ggplot}\NormalTok{(combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes,}\AttributeTok{fill=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Number of Entries per Class"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Entries"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-11-1.pdf}

There seems to be a row that has a null entry in the Classes column.
Upon further inspection, this is most likely due to an error in data
entry. In the 169th row of the original dataset, DC is equal to 14.6 9.
All the entries to the right then seem to be shifted to the left as FWI
is equal to ``fire''. So the mistake must be fixed before proceeding.
The most straightforward way of doing this is by fixing the error in the
original csv file itself and repeating the procedure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#loading the new dataset}
\NormalTok{ForestFires}\OtherTok{=}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"forest\_fires\_dataset\_fixed\_entry.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{na.strings=}\StringTok{"?"}\NormalTok{)}

\CommentTok{\#seperating by region}
\NormalTok{fires\_cordillera }\OtherTok{=}\NormalTok{ ForestFires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{122}\NormalTok{,]}
\NormalTok{fires\_newhudson }\OtherTok{=}\NormalTok{ ForestFires[}\DecValTok{126}\SpecialCharTok{:}\DecValTok{247}\NormalTok{,]}

\CommentTok{\#combining day, month and year and removing them}
\NormalTok{fires\_newhudson}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{=} \FunctionTok{paste}\NormalTok{(fires\_newhudson}\SpecialCharTok{$}\NormalTok{year,fires\_newhudson}\SpecialCharTok{$}\NormalTok{month,fires\_newhudson}\SpecialCharTok{$}\NormalTok{day,}\AttributeTok{sep=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fires\_cordillera}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{=} \FunctionTok{paste}\NormalTok{(fires\_cordillera}\SpecialCharTok{$}\NormalTok{year,fires\_cordillera}\SpecialCharTok{$}\NormalTok{month,fires\_cordillera}\SpecialCharTok{$}\NormalTok{day,}\AttributeTok{sep=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fires\_newhudson }\OtherTok{=}\NormalTok{ fires\_newhudson[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{15}\NormalTok{]}
\NormalTok{fires\_cordillera }\OtherTok{=}\NormalTok{ fires\_cordillera[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{15}\NormalTok{]}

\CommentTok{\#giving each dataframe a Region column}
\NormalTok{fires\_newhudson}\SpecialCharTok{$}\NormalTok{Region }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\StringTok{"New Hudson"}\NormalTok{)}
\NormalTok{fires\_cordillera}\SpecialCharTok{$}\NormalTok{Region }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\StringTok{"Cordillera"}\NormalTok{)}

\CommentTok{\#trimming the Classes columns}
\NormalTok{fires\_cordillera}\SpecialCharTok{$}\NormalTok{Classes }\OtherTok{=} \FunctionTok{trimws}\NormalTok{(fires\_cordillera}\SpecialCharTok{$}\NormalTok{Classes)}
\NormalTok{fires\_newhudson}\SpecialCharTok{$}\NormalTok{Classes }\OtherTok{=} \FunctionTok{trimws}\NormalTok{(fires\_newhudson}\SpecialCharTok{$}\NormalTok{Classes)}

\CommentTok{\#combing the two dataframes}
\NormalTok{combined\_fires }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(fires\_cordillera,fires\_newhudson)}

\CommentTok{\#entries in the Classes column}
\NormalTok{combined }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes,}\AttributeTok{fill=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Combined Entries"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Entries"}\NormalTok{)}
\NormalTok{separate }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes,}\AttributeTok{fill=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Region) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Separated Entries by Region"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Entries"}\NormalTok{)}
\FunctionTok{plot\_grid}\NormalTok{(combined,separate)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-12-1.pdf}

There are now two classes as is meant to be. Additionally, the balance
isn't extremely uneven and so there should be little worry in that
respect. The balance between classes is most uneven in New Hudson with
almost double the entries being part of the ``fire'' class. This may
prove to be significant later on and this point can be returned to in
the future if this proves so. To put all this numerically:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fire\_C }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(fires\_cordillera[fires\_cordillera}\SpecialCharTok{$}\NormalTok{Classes }\SpecialCharTok{==} \StringTok{"fire"}\NormalTok{,])}
\NormalTok{notfire\_C }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(fires\_cordillera[fires\_cordillera}\SpecialCharTok{$}\NormalTok{Classes }\SpecialCharTok{==} \StringTok{"not fire"}\NormalTok{,])}
\NormalTok{fire\_N }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(fires\_newhudson[fires\_newhudson}\SpecialCharTok{$}\NormalTok{Classes }\SpecialCharTok{==} \StringTok{"fire"}\NormalTok{,])}
\NormalTok{notfire\_N }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(fires\_newhudson[fires\_newhudson}\SpecialCharTok{$}\NormalTok{Classes }\SpecialCharTok{==} \StringTok{"not fire"}\NormalTok{,])}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}fire\textquotesingle{} entries in Cordillera\textquotesingle{}s data is \%s"}\NormalTok{,fire\_C)}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}not fire\textquotesingle{} entries in Cordillera\textquotesingle{}s data is \%s"}\NormalTok{,notfire\_C)}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}fire\textquotesingle{} entries in New Hudson\textquotesingle{}s data is \%s"}\NormalTok{,fire\_N)}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}not fire\textquotesingle{} entries in New Hudson\textquotesingle{}s data is \%s"}\NormalTok{,notfire\_N)}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}fire\textquotesingle{} entries in both is \%d"}\NormalTok{,fire\_C}\SpecialCharTok{+}\NormalTok{fire\_N)}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"The number of \textquotesingle{}not fire\textquotesingle{} entries in both is \%d"}\NormalTok{,notfire\_C}\SpecialCharTok{+}\NormalTok{notfire\_N)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "The number of 'fire' entries in Cordillera's data is 59"
## [1] "The number of 'not fire' entries in Cordillera's data is 63"
## [1] "The number of 'fire' entries in New Hudson's data is 79"
## [1] "The number of 'not fire' entries in New Hudson's data is 43"
## [1] "The number of 'fire' entries in both is 138"
## [1] "The number of 'not fire' entries in both is 106"
\end{verbatim}

\subsection{Encoding}\label{encoding}

It must be brought to attention that the quantitative data entries are
currently characterized as characters. It would be best to ensure this
data is converted to numeric before proceeding. Additionally, the Date
entries are converted to the Date type.

There are is now a qualitative predictor added to the dataset being the
Region column. This predictor is converted to the factor type in order
to be handled accordingly. The same is done for the Classes column in
order for it to be handled properly by the models later on.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#converting the entries of the combined dataframe to their relevant type}
\NormalTok{combined\_fires }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{mutate\_all}\NormalTok{(combined\_fires[}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(x))),}\FunctionTok{as.factor}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Classes), }\FunctionTok{as.Date}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Date),}\FunctionTok{as.factor}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Region))}

\CommentTok{\#renaming the Date, Region and Classes Column}
\FunctionTok{names}\NormalTok{(combined\_fires)[}\FunctionTok{names}\NormalTok{(combined\_fires) }\SpecialCharTok{==} \StringTok{"as.Date(combined\_fires$Date)"}\NormalTok{] }\OtherTok{=} \StringTok{"Date"}
\FunctionTok{names}\NormalTok{(combined\_fires)[}\FunctionTok{names}\NormalTok{(combined\_fires) }\SpecialCharTok{==} \StringTok{"as.factor(combined\_fires$Classes)"}\NormalTok{] }\OtherTok{=} \StringTok{"Classes"}
\FunctionTok{names}\NormalTok{(combined\_fires)[}\FunctionTok{names}\NormalTok{(combined\_fires) }\SpecialCharTok{==} \StringTok{"as.factor(combined\_fires$Region)"}\NormalTok{] }\OtherTok{=} \StringTok{"Region"}
\end{Highlighting}
\end{Shaded}

\subsection{Outliers}\label{outliers}

The dataset is riddled with outliers. We can generate a summary of the
Rain predictor to get a better look at this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Rain)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.7607  0.5000 16.8000
\end{verbatim}

We see in the Rain column, the maximal value is 16.8 whereas the mean of
the set lies around 0.76. This implies that the maximum value is likely
an outlier for this predictor. This can be better seen in the following
plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{Rain)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=} \StringTok{"Rain Level per Day"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-16-1.pdf}

A box plot can be generated to get a better visualization of these
outliers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\AttributeTok{mapping=}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes, }\AttributeTok{y=}\NormalTok{Rain)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=} \StringTok{"Box and Whiskers Plot for Rain"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-17-1.pdf}

To show that this is not only the case for the Rain predictor, we can
generate another box plot for DMC.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\AttributeTok{mapping=}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Classes, }\AttributeTok{y=}\NormalTok{DMC)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Box and Whiskers Plot for DMC"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-18-1.pdf}

There are quite a few outliers scattered throughout the dataset.
However, it is unclear as to whether the presence of these outliers is
even an issue. For example, it seems logical that on a day with high
levels of rain, there should not be any forest fires. In fact, this is
immediately visible from the generated box plot. We see that in the
column designated for ``fire entries, rain level is generally much
lower. For this reason throughout the rest of this work, outliers will
be maintained.

\subsection{Normalization and
Standardization}\label{normalization-and-standardization}

Obviously, due to the large presence of outliers in our data,
normalization is likely going to be ineffective. However, we still need
to consider standardization. In order to see if this technique would be
viable over our dataset, we must determine if our data is relatively
normally distributed. We can begin by simply generating a density plot
for one of the predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Temperature)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Density Plot of Temperature"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-19-1.pdf}

Just from a quick visual analysis, the data certainly looks bell-shaped.
However, a slightly more accurate analysis is required. In order to do
this, we will generate quantile-quantile (Q-Q) plots for each numerical
predictor.

A Q-Q plot, is a scatterplot created by plotting two sets of quantiles
or percentiles against each other. If we want to show that the data is
normally distributed we would plot quantiles gathered from our data
against theoretical normally distributed quantiles. If the data is
normally distributed, the quantiles will be comparable and will
therefore fit a straight line.

We can begin by plotting the Q-Q plot for Temperature.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Temperature)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Temperature Q{-}Q Plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-20-1.pdf}

We observe that the points quite closely fit a straight line. It is
therefore safe to assume that Temperature is normally distributed.
However, we must also check the other predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Q2 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ RH)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"RH Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q3 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Ws)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Ws Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q4 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Rain)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Rain Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q5 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ FFMC)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"FFMC Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q6 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ DMC)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"DMC Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q7 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ DC)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"DC Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q8 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ ISI)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"ISI Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q9 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ BUI)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"BUI Q{-}Q Plot"}\NormalTok{)}
\NormalTok{Q10 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ FWI)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"FWI Q{-}Q Plot"}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-21-1.pdf}

There are a couple predictors that seem to be normally distributed as
well. RH and Ws both closely fit a straight line. Additionally, other
variables lime ISI and BUI aren't necessarily very far off either.

It may not be the case that all of the data present is normally
distributed, but it seems that it may be close enough. Therefore, the
dataset will be standardized in order to at least improve the
performance of normally distributed predictors. The other predictors
will not be detrimentally affected by this choice anyways.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#making a copy}
\NormalTok{uns\_combined\_fires }\OtherTok{=}\NormalTok{ combined\_fires}

\NormalTok{combined\_fires }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{scale}\NormalTok{(combined\_fires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]),combined\_fires[}\DecValTok{11}\SpecialCharTok{:}\DecValTok{13}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\section{Exploratory Data Analysis}\label{exploratory-data-analysis}

It is first desirable to get a general idea of the data that we are
working with. A summary of the data containing important information can
be generated. An example of why this can be important is by taking a
look at Rain. The median of this predictor is 0 and the mean is quite
low as well. This indicates that the entries in this column are more
heavily weighted towards zero. Thinking about this logically, it makes
sense for there to be little rain during the months in which this data
was gathered (mainly Summer). However, on the flip side we expect that
the presence of rain should indicate no fire. This makes Rain a point of
question and interest. it becomes a question of whether the predictor is
insignificant due to the time period chosen or whether it will play a
larger role than expected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(combined\_fires)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Temperature             RH                 Ws               Rain        
##  Min.   :-2.79928   Min.   :-2.75047   Min.   :-3.3820   Min.   :-0.3804  
##  1st Qu.:-0.59775   1st Qu.:-0.66772   1st Qu.:-0.5352   1st Qu.:-0.3804  
##  Median :-0.04737   Median : 0.07132   Median :-0.1794   Median :-0.3804  
##  Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  
##  3rd Qu.: 0.77820   3rd Qu.: 0.75997   3rd Qu.: 0.5323   3rd Qu.:-0.1304  
##  Max.   : 2.70454   Max.   : 1.88532   Max.   : 4.8025   Max.   : 8.0221  
##       FFMC              DMC                DC               ISI         
##  Min.   :-3.4377   Min.   :-1.1298   Min.   :-0.8901   Min.   :-1.1457  
##  1st Qu.:-0.4054   1st Qu.:-0.7174   1st Qu.:-0.7563   1st Qu.:-0.8087  
##  Median : 0.3914   Median :-0.2727   Median :-0.3399   Median :-0.3032  
##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
##  3rd Qu.: 0.7262   3rd Qu.: 0.4913   3rd Qu.: 0.3961   3rd Qu.: 0.6114  
##  Max.   : 1.2633   Max.   : 4.1419   Max.   : 3.5933   Max.   : 3.4275  
##       BUI               FWI              Classes         Date           
##  Min.   :-1.0966   Min.   :-0.9490   fire    :138   Min.   :2012-06-01  
##  1st Qu.:-0.7516   1st Qu.:-0.8547   not fire:106   1st Qu.:2012-07-01  
##  Median :-0.2974   Median :-0.3499                  Median :2012-07-31  
##  Mean   : 0.0000   Mean   : 0.0000                  Mean   :2012-07-31  
##  3rd Qu.: 0.4120   3rd Qu.: 0.5823                  3rd Qu.:2012-08-31  
##  Max.   : 3.6141   Max.   : 3.2377                  Max.   :2012-09-30  
##         Region   
##  Cordillera:122  
##  New Hudson:122  
##                  
##                  
##                  
## 
\end{verbatim}

\subsection{Pair-wise Analysis}\label{pair-wise-analysis}

One preliminary way of looking at the relationship between variables is
to set them against each other in a scatter plot. As an example, we can
take Temperature and Date.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Date, }\AttributeTok{y =}\NormalTok{ Temperature)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Region, }\AttributeTok{shape =}\NormalTok{ Region)) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Temperature per Day"}\NormalTok{,}\AttributeTok{subtitle =} \StringTok{"in Cordillera and New Hudson"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Day"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Region"}\NormalTok{, }\AttributeTok{shape =} \StringTok{"Region"}\NormalTok{ ) }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Region))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'
\end{verbatim}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-24-1.pdf}

The above showcases the fluctuations in temperature over the days
provided in the dataset. Smooth fits are used to show the general trend.
We can immediately identify a trend which makes sense here being that
maximum temperature increases up to August and starts decreasing
afterwards. This trend is seasonal and logical. Additionally, applying
rationality to this pair of predictors, we might expect more forest
fires to occur during the dates where temperature is warmer. This can be
studied using the following plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Date, }\AttributeTok{y =}\NormalTok{ Temperature)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Classes, }\AttributeTok{shape =}\NormalTok{ Region)) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Temperature per Day"}\NormalTok{,}\AttributeTok{subtitle =} \StringTok{"in Cordillera and New Hudson"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Day"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Class"}\NormalTok{, }\AttributeTok{shape =} \StringTok{"Region"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-25-1.pdf}

We can determine visually from the plot that most points that pertain to
the ``fire'' class lie above points pertaining to the ``not fire''
class. This falls in line with our prediction above. It is more likely
for a forest fire to occur when temperature is warmer rather than
cooler. Additionally, we note that there are hardly any forest fires on
days where temperature was below 27 degrees. While these data points may
have been considered outliers prior to this knowledge, it must now be
reconsidered that these points contain valuable information in being
able to predict a forest fire taking place.

This interpretation can even be quantized further by showcasing the data
in a density plot and differentiating by class.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Temperature)) }\SpecialCharTok{+} \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Classes)) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Density Plot of Temperature"}\NormalTok{,}\AttributeTok{subtitle =} \StringTok{"differentiating by Class"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Temperature"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-26-1.pdf}

We note that there is a clearly a greater density of ``fire'' entries at
higher temperatures and a greater density of ``not fire'' entries at
lower temperatures. There is a large intersection between the two curves
meaning that the separation is not completely clear, but the trend is
certainly visible. The density plots of other predictors are shown.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{G1 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Ws)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Density Plot of Wind Speed"}\NormalTok{,}\AttributeTok{subtitle =} \StringTok{"differentiating by Class"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Wind Speed"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Class"}\NormalTok{)}
\NormalTok{G2 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ combined\_fires, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ISI)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Classes)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Density Plot of ISI"}\NormalTok{,}\AttributeTok{subtitle =} \StringTok{"differentiating by Class"}\NormalTok{, }\AttributeTok{x=}\StringTok{"ISI"}\NormalTok{, }\AttributeTok{color=}\StringTok{"Class"}\NormalTok{)}
\FunctionTok{plot\_grid}\NormalTok{(G1,G2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-27-1.pdf}

In the case of wind speed, ``fire'' and ``not fire'' values seem to
occur in very similar ranges. This may indicate that this will not be a
great predictor later on for the models. It could also not be the case
as the relationship may be more intricate.

On the other hand, when looking at ISI, the two classes become almost
entirely separate. A higher ISI seems to be linked with forest fires.
This could indicate ISI being a strong predictor especially for models
estimating Baye's decision boundary.

\subsection{Scatter Plot Matrix and
Correlation}\label{scatter-plot-matrix-and-correlation}

Another way to get a preliminary idea of the way that predictors
interact with each other is by generating a scatterplot matrix. In doing
this, it can be made immediately obvious which variables are strongly
correlated and which are not. However, we must note that this only
accounts for correlation between two variables and not more. This will
be accounted for later.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(combined\_fires)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-28-1.pdf}

There is much going on and it would be almost pointless trying to
interpret every single plot. There are quite a few obvious positive
correlations such as that between DMC or BUI or that between ISI and
FWI. This is notably the case mostly with the FWI components. Seeing as
they are built upon each other (for example, BUI is built on DMC and DC.
FWI is dependent on all 5 components). This information may prove useful
in the future when discerning trends or interpreting results.

To get a better picture of this, a matrix with the correlation of each
pair is generated.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(combined\_fires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Temperature     RH     Ws   Rain   FFMC    DMC     DC    ISI    BUI
## Temperature       1.000 -0.654 -0.278 -0.327  0.677  0.483  0.370  0.606  0.456
## RH               -0.654  1.000  0.236  0.223 -0.646 -0.405 -0.220 -0.688 -0.350
## Ws               -0.278  0.236  1.000  0.170 -0.163 -0.001  0.076  0.012  0.030
## Rain             -0.327  0.223  0.170  1.000 -0.544 -0.289 -0.297 -0.348 -0.299
## FFMC              0.677 -0.646 -0.163 -0.544  1.000  0.602  0.504  0.741  0.590
## DMC               0.483 -0.405 -0.001 -0.289  0.602  1.000  0.875  0.678  0.982
## DC                0.370 -0.220  0.076 -0.297  0.504  0.875  1.000  0.504  0.942
## ISI               0.606 -0.688  0.012 -0.348  0.741  0.678  0.504  1.000  0.641
## BUI               0.456 -0.350  0.030 -0.299  0.590  0.982  0.942  0.641  1.000
## FWI               0.567 -0.580  0.034 -0.325  0.691  0.875  0.737  0.922  0.857
##                FWI
## Temperature  0.567
## RH          -0.580
## Ws           0.034
## Rain        -0.325
## FFMC         0.691
## DMC          0.875
## DC           0.737
## ISI          0.922
## BUI          0.857
## FWI          1.000
\end{verbatim}

We note from this that matrix that BUI has a couple strong correlations
with some other predictors. It may prove wise therefore to later on look
at the Variance Inflation Factor (VIF) of this predictor to get an idea
of not only its pair-wise correlation but rather it's general
dataset-wide correlation. The reason that we are interested in this is
because strong correlation can often be dangerous and can skew results.
This occurs in the form of an increase in the correlated predictor's
standard error and therefore an increase in its p-value. A strong
correlation is all it takes for the hypothesis test to lose its power in
being able to predict the significance of a predictor. This idea will be
important when analyzing the results of the logistic regression output.

\section{Feature Selection}\label{feature-selection}

\subsection{Best Subset Selection}\label{best-subset-selection}

Typically, subset selection is used for regression models. However,
there are still some useful pieces of information we can gather from it.

Seeing as the dataset being used is not extremely massive we can afford
to conduct a best subset selection algorithm. We are initially
interested in calculating the exhaustive selection for all of the
predictors as well as the calculated outputs such as:

\begin{itemize}
\item
  Bayesian Information Criterion (BIC): A measure of the trade-off
  between model fit and the complexity of the model. A lower BIC is
  favorable.
\item
  Residual Sum of Squares (RSS): The sum of the squares of residuals. It
  depends on the problem at hand, but a generally lower RSS is favorable
  as the model is fit better. However, it must be noted that RSS is
  non-increasing and will tend to decrease no matter what.
\item
  Adjusted R\(^2\): A metric that describes the percentage of variance
  in the target field explainable by the inputs. This metric will also
  tend to plateau as more predictors are added.
\item
  Mallow's C\(_p\): Another method that assesses the fit of a model.
  Similar to BIC, a lower C\(_p\) indicates a more precise model.
\end{itemize}

The exhaustive selection algorithm must first be run on the data. we set
nvmax = 12 in order to tell the model to calculate the best subset for
every size subset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.best }\OtherTok{=} \FunctionTok{regsubsets}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ combined\_fires, }\AttributeTok{nvmax =} \DecValTok{12}\NormalTok{)}
\NormalTok{reg.summary }\OtherTok{=} \FunctionTok{summary}\NormalTok{(regfit.best)}
\NormalTok{reg.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Subset selection object
## Call: regsubsets.formula(Classes ~ ., data = combined_fires, nvmax = 12)
## 12 Variables  (and intercept)
##                  Forced in Forced out
## Temperature          FALSE      FALSE
## RH                   FALSE      FALSE
## Ws                   FALSE      FALSE
## Rain                 FALSE      FALSE
## FFMC                 FALSE      FALSE
## DMC                  FALSE      FALSE
## DC                   FALSE      FALSE
## ISI                  FALSE      FALSE
## BUI                  FALSE      FALSE
## FWI                  FALSE      FALSE
## Date                 FALSE      FALSE
## RegionNew Hudson     FALSE      FALSE
## 1 subsets of each size up to 12
## Selection Algorithm: exhaustive
##           Temperature RH  Ws  Rain FFMC DMC DC  ISI BUI FWI Date
## 1  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " " " " " 
## 2  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " "*" " " 
## 3  ( 1 )  " "         "*" " " " "  "*"  " " " " "*" " " " " " " 
## 4  ( 1 )  " "         "*" " " " "  "*"  "*" " " " " " " "*" " " 
## 5  ( 1 )  " "         "*" " " "*"  "*"  "*" " " " " " " "*" " " 
## 6  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 7  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 8  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " "*" "*" " " 
## 9  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" " " 
## 10  ( 1 ) " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" "*" 
## 11  ( 1 ) " "         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
## 12  ( 1 ) "*"         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
##           RegionNew Hudson
## 1  ( 1 )  " "             
## 2  ( 1 )  " "             
## 3  ( 1 )  " "             
## 4  ( 1 )  " "             
## 5  ( 1 )  " "             
## 6  ( 1 )  " "             
## 7  ( 1 )  "*"             
## 8  ( 1 )  "*"             
## 9  ( 1 )  "*"             
## 10  ( 1 ) "*"             
## 11  ( 1 ) "*"             
## 12  ( 1 ) "*"
\end{verbatim}

Now the relevant metrics can be picked out and plotted against the
number of elements in each step. This will give an idea of the optimal
subset to choose.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating a dataframe housing relevant metrics}
\NormalTok{metrics }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{adjr2 =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{adjr2,}\AttributeTok{cp =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{cp,}\AttributeTok{bic =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{bic,}\AttributeTok{rss =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{rss)}
\NormalTok{metrics}\SpecialCharTok{$}\NormalTok{n }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(metrics)}

\CommentTok{\#plot for RSS}
\NormalTok{RSS }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ rss, }\AttributeTok{color =}\NormalTok{ rss)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"RSS"}\NormalTok{, }\AttributeTok{color =} \StringTok{"RSS"}\NormalTok{)}

\CommentTok{\#plot for Adjusted R Squared}
\NormalTok{ADJR2 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ adjr2, }\AttributeTok{color =}\NormalTok{ adjr2)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"ADJR2"}\NormalTok{, }\AttributeTok{color=}\StringTok{"ADJR2"}\NormalTok{)}

\CommentTok{\#plot for BIC}
\NormalTok{BIC }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ bic, }\AttributeTok{color =}\NormalTok{ bic)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"BIC"}\NormalTok{, }\AttributeTok{color=}\StringTok{"BIC"}\NormalTok{)}

\CommentTok{\#plot for Cp}
\NormalTok{CP }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ cp, }\AttributeTok{color =}\NormalTok{ cp)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y=}\StringTok{"CP"}\NormalTok{, }\AttributeTok{color=}\StringTok{"CP"}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(RSS,ADJR2,BIC,CP)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-31-1.pdf}

Minimum BIC occurs at a subset of 4 elements whereas minimum C\(_p\)
occurs at a subset of size 6. This indicates that a subset containing
less than 4 or greater than 6 elements may prove insufficient. BIC will
be chosen as the stronger indicator and so the best subset containing 4
elements will be chosen as this offers a low C\(_p\), BIC, RSS, and a
high Adjusted R\(^2\).

This leaves us with a subset consisting of RH, FFMC, DMC and FWI.

Before leaving this section, it is worth noting that in the case of a
subset of size 3, ISI is chosen as one of the predictors. As soon as the
size increases, it is removed. It will be shown later that ISI is a
strong predictor by itself and as such it is interesting that it appears
and disappears like this.

\subsection{Backward and Forward Subset
Selection}\label{backward-and-forward-subset-selection}

When running the backward subset selection,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.bwd }\OtherTok{=} \FunctionTok{regsubsets}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ combined\_fires, }\AttributeTok{nvmax =} \DecValTok{12}\NormalTok{, }\AttributeTok{method=}\StringTok{"backward"}\NormalTok{)}
\NormalTok{reg.summary }\OtherTok{=} \FunctionTok{summary}\NormalTok{(regfit.bwd)}
\NormalTok{reg.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Subset selection object
## Call: regsubsets.formula(Classes ~ ., data = combined_fires, nvmax = 12, 
##     method = "backward")
## 12 Variables  (and intercept)
##                  Forced in Forced out
## Temperature          FALSE      FALSE
## RH                   FALSE      FALSE
## Ws                   FALSE      FALSE
## Rain                 FALSE      FALSE
## FFMC                 FALSE      FALSE
## DMC                  FALSE      FALSE
## DC                   FALSE      FALSE
## ISI                  FALSE      FALSE
## BUI                  FALSE      FALSE
## FWI                  FALSE      FALSE
## Date                 FALSE      FALSE
## RegionNew Hudson     FALSE      FALSE
## 1 subsets of each size up to 12
## Selection Algorithm: backward
##           Temperature RH  Ws  Rain FFMC DMC DC  ISI BUI FWI Date
## 1  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " " " " " 
## 2  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " "*" " " 
## 3  ( 1 )  " "         "*" " " " "  "*"  " " " " " " " " "*" " " 
## 4  ( 1 )  " "         "*" " " " "  "*"  "*" " " " " " " "*" " " 
## 5  ( 1 )  " "         "*" " " "*"  "*"  "*" " " " " " " "*" " " 
## 6  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 7  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 8  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " "*" "*" " " 
## 9  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" " " 
## 10  ( 1 ) " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" "*" 
## 11  ( 1 ) " "         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
## 12  ( 1 ) "*"         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
##           RegionNew Hudson
## 1  ( 1 )  " "             
## 2  ( 1 )  " "             
## 3  ( 1 )  " "             
## 4  ( 1 )  " "             
## 5  ( 1 )  " "             
## 6  ( 1 )  " "             
## 7  ( 1 )  "*"             
## 8  ( 1 )  "*"             
## 9  ( 1 )  "*"             
## 10  ( 1 ) "*"             
## 11  ( 1 ) "*"             
## 12  ( 1 ) "*"
\end{verbatim}

and the forward subset selection,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.fwd }\OtherTok{=} \FunctionTok{regsubsets}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ combined\_fires, }\AttributeTok{nvmax =} \DecValTok{12}\NormalTok{, }\AttributeTok{method=}\StringTok{"forward"}\NormalTok{)}
\NormalTok{reg.summary }\OtherTok{=} \FunctionTok{summary}\NormalTok{(regfit.fwd)}
\NormalTok{reg.summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Subset selection object
## Call: regsubsets.formula(Classes ~ ., data = combined_fires, nvmax = 12, 
##     method = "forward")
## 12 Variables  (and intercept)
##                  Forced in Forced out
## Temperature          FALSE      FALSE
## RH                   FALSE      FALSE
## Ws                   FALSE      FALSE
## Rain                 FALSE      FALSE
## FFMC                 FALSE      FALSE
## DMC                  FALSE      FALSE
## DC                   FALSE      FALSE
## ISI                  FALSE      FALSE
## BUI                  FALSE      FALSE
## FWI                  FALSE      FALSE
## Date                 FALSE      FALSE
## RegionNew Hudson     FALSE      FALSE
## 1 subsets of each size up to 12
## Selection Algorithm: forward
##           Temperature RH  Ws  Rain FFMC DMC DC  ISI BUI FWI Date
## 1  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " " " " " 
## 2  ( 1 )  " "         " " " " " "  "*"  " " " " " " " " "*" " " 
## 3  ( 1 )  " "         "*" " " " "  "*"  " " " " " " " " "*" " " 
## 4  ( 1 )  " "         "*" " " " "  "*"  "*" " " " " " " "*" " " 
## 5  ( 1 )  " "         "*" " " "*"  "*"  "*" " " " " " " "*" " " 
## 6  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 7  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " " " "*" " " 
## 8  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " " " "*" "*" " " 
## 9  ( 1 )  " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" " " 
## 10  ( 1 ) " "         "*" "*" "*"  "*"  "*" " " "*" "*" "*" "*" 
## 11  ( 1 ) " "         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
## 12  ( 1 ) "*"         "*" "*" "*"  "*"  "*" "*" "*" "*" "*" "*" 
##           RegionNew Hudson
## 1  ( 1 )  " "             
## 2  ( 1 )  " "             
## 3  ( 1 )  " "             
## 4  ( 1 )  " "             
## 5  ( 1 )  " "             
## 6  ( 1 )  " "             
## 7  ( 1 )  "*"             
## 8  ( 1 )  "*"             
## 9  ( 1 )  "*"             
## 10  ( 1 ) "*"             
## 11  ( 1 ) "*"             
## 12  ( 1 ) "*"
\end{verbatim}

We note that the subset matrices are equivalent. Therefore, studying
either of these methods is sufficient enough to determine an optimal
subset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating a dataframe housing relevant metrics}
\NormalTok{metrics }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{adjr2 =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{adjr2,}\AttributeTok{cp =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{cp,}\AttributeTok{bic =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{bic,}\AttributeTok{rss =}\NormalTok{ reg.summary}\SpecialCharTok{$}\NormalTok{rss)}
\NormalTok{metrics}\SpecialCharTok{$}\NormalTok{n }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(metrics)}

\CommentTok{\#plot for RSS}
\NormalTok{RSS }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ rss, }\AttributeTok{color =}\NormalTok{ rss)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"RSS"}\NormalTok{, }\AttributeTok{color =} \StringTok{"RSS"}\NormalTok{)}

\CommentTok{\#plot for Adjusted R Square}
\NormalTok{ADJR2 }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ adjr2, }\AttributeTok{color =}\NormalTok{ adjr2)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"ADJR2"}\NormalTok{, }\AttributeTok{color=}\StringTok{"ADJR2"}\NormalTok{)}

\CommentTok{\#plot for BIC}
\NormalTok{BIC }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ bic, }\AttributeTok{color =}\NormalTok{ bic)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"BIC"}\NormalTok{, }\AttributeTok{color=}\StringTok{"BIC"}\NormalTok{)}

\CommentTok{\#plot for Cp}
\NormalTok{CP }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ metrics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ cp, }\AttributeTok{color =}\NormalTok{ cp)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"number of predictors"}\NormalTok{, }\AttributeTok{y=}\StringTok{"CP"}\NormalTok{, }\AttributeTok{color=}\StringTok{"CP"}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(RSS,ADJR2,BIC,CP)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-34-1.pdf}

We note that the shape of our curves differ slightly from the best
subset selection case. However, the main trends remain consistent. The
minimum BIC is at 4 and the minimum C\(_p\) is 6. We will stick to the
decision of prioritizing BIC and choose 4 as our optimal subset size.

This leaves us with the same subset as in the best subset selection case
and therefore we can merge the two.

\subsection{Principal Component Analysis
(PCA)}\label{principal-component-analysis-pca}

Another effective technique to discern important predictors is Principal
Component Analysis. This method only works on numerical values and so
qualitative predictors must be omitted. This is a potential limitation,
but seeing as Region only shows up in the subset later on, we can assume
that its omission will not be detrimental.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_result }\OtherTok{=} \FunctionTok{princomp}\NormalTok{(combined\_fires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\FunctionTok{summary}\NormalTok{(pca\_result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Importance of components:
##                           Comp.1    Comp.2     Comp.3     Comp.4     Comp.5
## Standard deviation     2.3862105 1.2559583 0.96009093 0.89273903 0.62321040
## Proportion of Variance 0.5717433 0.1583923 0.09255679 0.08002627 0.03899895
## Cumulative Proportion  0.5717433 0.7301355 0.82269232 0.90271860 0.94171755
##                            Comp.6     Comp.7      Comp.8      Comp.9
## Standard deviation     0.49752443 0.47011400 0.303316371 0.126723173
## Proportion of Variance 0.02485492 0.02219167 0.009237943 0.001612485
## Cumulative Proportion  0.96657247 0.98876414 0.998002081 0.999614565
##                             Comp.10
## Standard deviation     0.0619560328
## Proportion of Variance 0.0003854347
## Cumulative Proportion  1.0000000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#showing the results for the first four principal components}
\NormalTok{pca\_result}\SpecialCharTok{$}\NormalTok{loadings[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Comp.1      Comp.2      Comp.3       Comp.4
## Temperature  0.29842142  0.35016413  0.11288952  0.103217979
## RH          -0.27684872 -0.40231788 -0.41407257 -0.045781958
## Ws          -0.03923363 -0.52789351  0.46415351 -0.621961163
## Rain        -0.19640308 -0.21454466  0.61241076  0.642444611
## FFMC         0.34881451  0.23385673 -0.03126155 -0.236064975
## DMC          0.37430851 -0.26897780 -0.09791590  0.205127314
## DC           0.32900078 -0.37895685 -0.25577553  0.161633611
## ISI          0.36407003  0.07804613  0.31782669 -0.166776551
## BUI          0.36983407 -0.31666663 -0.15213992  0.189537798
## FWI          0.39349946 -0.11751240  0.15864473  0.003314835
\end{verbatim}

The eigenvalues of each generated component are given along with the
proportion of variance explainable by each component. We see that the
first principal component explains roughly 57\% of the variance. An
additional 16\% is explained by the second, and 9\% by the third.
Cumulatively, these three components explain 82\% of the variance in the
data. This means that the first three principal components likely
already do a good job of describing the dataset. Add on a fourth
principal component and the percentage jumps up to 90. A choice of 3 or
4 principal components will be satisfactory.

A scree plot is just the linear plot of the eigenvalues of the principal
components. The scree plot for the generated values is given below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_eig}\NormalTok{(pca\_result, }\AttributeTok{addlabels =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-36-1.pdf}

The idea that 3 or 4 dimensions is sufficient in describing variance in
the dataset is reiterated by the scree plot. To look at the predictors
more closely, a biplot is generated. This type of plot is useful in
visualizing the similarities and dissimilarities between variables. It
showcases the impact of each on the principal components.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_pca\_var}\NormalTok{(pca\_result, }\AttributeTok{col.var =} \StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-37-1.pdf}

Through this plot, much information is gathered. Firstly, the farther a
variable is from the origin, the more power it holds within principal
components. This means that Rain may not be as significant as say DMC.

Another piece of information gatherable from this plot is correlation.
Arrows closer together are more correlated. We see Rain and RH in the
third quadrant of the plot. This implies that the two variables are
closely correlated and that they are negatively correlated to the Class.
In other words, less rain indicates more fire and the same for humidity.
On the other end of the plot, we see a large block of components very
closely correlated to one another. This includes FWI and all the
components that it is composed of. along with Temperature. These
variables are positively correlated to the class and therefore an
increase in these values more likely implies an increase in fires.

To get a better understanding of the importance of each variable within
the components, a Cos2 graph is plotted. A low value implies that the
the given variable is not well represented by the component. A higher
value implies the opposite. The Cos2 plot is computed for the first
three principal components as these three offer a good explanation for
the variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cos2}\NormalTok{(pca\_result, }\AttributeTok{choice =} \StringTok{"var"}\NormalTok{, }\AttributeTok{axes =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-38-1.pdf}

We see that the variables that contributed the most to the first three
principal components were BUI, FWI, DMC, DC\ldots{} These are in face
the FWI components. If a subset of size 4 were to be taken then it would
compose of the first 4 variables in this plot. Combining the biplot and
the Cos2 plot, we obtain the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_pca\_var}\NormalTok{(pca\_result, }\AttributeTok{col.var =} \StringTok{"cos2"}\NormalTok{,}
            \AttributeTok{gradient.cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{, }\StringTok{"green"}\NormalTok{),}
            \AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-39-1.pdf}

Not much interpretation is gained from the combination of the two plots.
The weight of each variable is reiterated along with the similarity
between each.

\subsection{Recursive Feature Elimination
(RFE)}\label{recursive-feature-elimination-rfe}

RFE is similar to backwards subset selection. The entire set of features
is taken at first and one of the predictors is dropped. This process
repeats recursively until a stopping condition is met or until there are
no more predictors. The difference is in the method of ranking features.
In this case, Random Forest will be used to do so. The Random Forest
method will be discussed later on in greater detail. However, for now,
it offers another effective means of determining the most significant
predictors in the dataset.

Additionally, Cross Validation will be used to evaluate the model at
each step. Essentially, the data will be separated into segments one of
which will be omitted from training in order to be used to evaluate.
This evaluation process will be repeated a certain number of times and a
final accuracy will be reported. Based on this accuracy, we will be able
to determine the size of the subset and the elements within that offer
the optimal results.

A 5-fold cross validation is chosen with 5 repetitions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the tuning parameters}
\NormalTok{control\_rfe }\OtherTok{=} \FunctionTok{rfeControl}\NormalTok{(}\AttributeTok{functions =}\NormalTok{ rfFuncs, }\AttributeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\AttributeTok{repeats =} \DecValTok{5}\NormalTok{, }\AttributeTok{number =} \DecValTok{5}\NormalTok{)}

\CommentTok{\#computing the subset}
\NormalTok{result\_rfe }\OtherTok{=} \FunctionTok{rfe}\NormalTok{(}\AttributeTok{x=}\FunctionTok{cbind}\NormalTok{(combined\_fires[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{],combined\_fires[}\DecValTok{12}\SpecialCharTok{:}\DecValTok{13}\NormalTok{]), }\AttributeTok{y=}\NormalTok{combined\_fires}\SpecialCharTok{$}\NormalTok{Classes, }\AttributeTok{sizes =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{), }\AttributeTok{rfeControl =}\NormalTok{ control\_rfe)}
\NormalTok{result\_rfe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (5 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables Accuracy  Kappa AccuracySD KappaSD Selected
##          1   0.9696 0.9378    0.02464 0.05022         
##          2   0.9853 0.9699    0.01718 0.03497        *
##          3   0.9771 0.9533    0.02078 0.04226         
##          4   0.9779 0.9549    0.02127 0.04313         
##          5   0.9770 0.9532    0.02238 0.04568         
##          6   0.9746 0.9483    0.02317 0.04717         
##          7   0.9762 0.9517    0.02107 0.04287         
##          8   0.9762 0.9517    0.02107 0.04287         
##          9   0.9754 0.9499    0.02213 0.04517         
##         10   0.9762 0.9517    0.02107 0.04287         
##         11   0.9762 0.9517    0.02107 0.04287         
##         12   0.9754 0.9501    0.02130 0.04331         
## 
## The top 2 variables (out of 2):
##    ISI, FFMC
\end{verbatim}

Unlike the previous selection methods, this one presents a subset of
size 2 as its optimal result. Additionally, this subset contains ISI
which was not present in any of the other given subsets. However, it is
worth remembering that ISI did show up in best subset selection at size
3 and then disappeared again.

Now that features have been studied in greater depth and subsets have
been formed, it is possible to move on and create models for the
dataset. There are other methods that could be used here to reduce the
dimensionality of the dataset such as LASSO. However, there are already
enough subsets to work with so this can be left as an extension to the
project.

\section{Model Development}\label{model-development}

\subsection{Partitioning the Dataset}\label{partitioning-the-dataset}

This section begins with the splitting of the data into a training and a
testing set. 70\% of values are chosen at random for the training set
and the remaining 30\% are used for the test set. The reason a 70:30
split is chosen rather than an 80:20 split is because the 70:30 split
leaves a little more room for test data entries. In the 80:20 split,
there are only 49 test elements to work with, whereas in the 70:30
split, there are 72.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#creating the partition and splitting the data for standardized set}
\NormalTok{trainIndex }\OtherTok{=} \FunctionTok{createDataPartition}\NormalTok{(combined\_fires}\SpecialCharTok{$}\NormalTok{Classes, }\AttributeTok{p=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{list=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{trainData }\OtherTok{=}\NormalTok{ combined\_fires[trainIndex,]}
\NormalTok{testData }\OtherTok{=}\NormalTok{ combined\_fires[}\SpecialCharTok{{-}}\NormalTok{trainIndex,]}

\CommentTok{\#splitting the training data into dependent and independent variables for standardized set}
\NormalTok{trainY }\OtherTok{=}\NormalTok{ trainData[,}\DecValTok{11}\NormalTok{]}
\NormalTok{trainX }\OtherTok{=}\NormalTok{ trainData[,}\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]}
\NormalTok{testY }\OtherTok{=}\NormalTok{ testData[,}\DecValTok{11}\NormalTok{]}
\NormalTok{testX }\OtherTok{=}\NormalTok{ testData[,}\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]}

\CommentTok{\#creating the partition and splitting the data for unstandardized set}
\NormalTok{uns\_trainIndex }\OtherTok{=} \FunctionTok{createDataPartition}\NormalTok{(uns\_combined\_fires}\SpecialCharTok{$}\NormalTok{Classes, }\AttributeTok{p=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{list=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{uns\_trainData }\OtherTok{=}\NormalTok{ uns\_combined\_fires[uns\_trainIndex,]}
\NormalTok{uns\_testData }\OtherTok{=}\NormalTok{ uns\_combined\_fires[}\SpecialCharTok{{-}}\NormalTok{uns\_trainIndex,]}

\CommentTok{\#splitting the training data into dependent and independent variables for unstandardized set}
\NormalTok{uns\_trainY }\OtherTok{=}\NormalTok{ uns\_trainData[,}\DecValTok{11}\NormalTok{]}
\NormalTok{uns\_trainX }\OtherTok{=}\NormalTok{ uns\_trainData[,}\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]}
\NormalTok{uns\_testY }\OtherTok{=}\NormalTok{ uns\_testData[,}\DecValTok{11}\NormalTok{]}
\NormalTok{uns\_testX }\OtherTok{=}\NormalTok{ uns\_testData[,}\SpecialCharTok{{-}}\DecValTok{11}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\subsection{Logistic Regression}\label{logistic-regression}

The first model used to tackle the dataset is a Logistic Regression
model.

\subsubsection{Single Variable Logistic
Regression}\label{single-variable-logistic-regression}

Once we have the training and test set, we begin fitting the model. The
model will initially consider Wind Speed as the only independent
variable. Wind Speed was analysed in the exploratory analysis and there
was hardly any split between the Classes when looking at the density
plot. For this reason, it was predicted to not perform well in being
able to discern the two Classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Ws, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ Ws, family = binomial, data = trainData)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept) -0.25702    0.15378  -1.671   0.0947 .
## Ws           0.03584    0.16271   0.220   0.8257  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 235.62  on 171  degrees of freedom
## Residual deviance: 235.57  on 170  degrees of freedom
## AIC: 239.57
## 
## Number of Fisher Scoring iterations: 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in confusionMatrix.default(data = as.factor(glm.pred), reference =
## testData$Classes): Levels are not in the same order for reference and data.
## Refactoring data to match.
\end{verbatim}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41       31
##   not fire    0        0
##                                           
##                Accuracy : 0.5694          
##                  95% CI : (0.4473, 0.6857)
##     No Information Rate : 0.5694          
##     P-Value [Acc > NIR] : 0.5495          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar's Test P-Value : 7.118e-08       
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.5694          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.5694          
##          Detection Rate : 0.5694          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        'Positive' Class : fire            
## 
\end{verbatim}

The first talking point concerns this predictor's p-value. It is well
over 0.05 and therefore is likely insignificant with regard to Class.
Additionally, when computing the confusion matrix, all false values were
predicted to be true. This likely indicates a faulty probability
threshold but upon further testing that is not shown in this project,
this predictor is basically hopeless by itself. Recall is 1 as all true
positives were found. However, precision is only 0.5694. Overall
accuracy is about the same indicating that this model is only slightly
better than a random guess.

Another predictor of interest is ISI. This predictor shown a good amount
of separation between classes upon exploratory analysis. Therefore we
predict it should perform quite well. The ROC curve for this model is
shown below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.probs))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-43-1.pdf}

The AUC is 0.546 which is just above half and again reflects the idea
that this predictor is hardly any better than a random guess.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI , }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ ISI, family = binomial, data = trainData)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  -14.502      5.585  -2.597  0.00941 **
## ISI          -29.718     11.153  -2.665  0.00771 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 235.621  on 171  degrees of freedom
## Residual deviance:  17.274  on 170  degrees of freedom
## AIC: 21.274
## 
## Number of Fisher Scoring iterations: 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41        1
##   not fire    0       30
##                                          
##                Accuracy : 0.9861         
##                  95% CI : (0.925, 0.9996)
##     No Information Rate : 0.5694         
##     P-Value [Acc > NIR] : <2e-16         
##                                          
##                   Kappa : 0.9716         
##                                          
##  Mcnemar's Test P-Value : 1              
##                                          
##             Sensitivity : 1.0000         
##             Specificity : 0.9677         
##          Pos Pred Value : 0.9762         
##          Neg Pred Value : 1.0000         
##              Prevalence : 0.5694         
##          Detection Rate : 0.5694         
##    Detection Prevalence : 0.5833         
##       Balanced Accuracy : 0.9839         
##                                          
##        'Positive' Class : fire           
## 
\end{verbatim}

It is indeed the case that ISI performs very well over the dataset. With
a p-value well under 0.05, the feature is very likely significant. It
showcases an accuracy of 0.9861 as well as a Recall of 1 and precision
of 0.976. This feature is clearly a very strong predictor in the
logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.probs))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-45-1.pdf}

The AUC for this model is 1. It is probably not exactly equal to 1 but
it is likely very close.

\subsubsection{Mulrivariate Logistic
Regression}\label{mulrivariate-logistic-regression}

We will now compute the logistic regression with all predictors followed
by the best subsets generated in the Feature Selection section.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: algorithm did not converge
\end{verbatim}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ ., family = binomial, data = trainData)
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(>|z|)
## (Intercept)      -4.864e+03  4.984e+06  -0.001    0.999
## Temperature       5.604e+01  9.098e+03   0.006    0.995
## RH               -5.681e+00  1.348e+04   0.000    1.000
## Ws                1.307e+01  1.937e+04   0.001    0.999
## Rain             -7.756e+01  4.406e+04  -0.002    0.999
## FFMC             -1.994e+02  2.045e+05  -0.001    0.999
## DMC               1.357e+02  6.397e+04   0.002    0.998
## DC               -1.189e+02  2.758e+04  -0.004    0.997
## ISI              -3.791e+02  1.529e+05  -0.002    0.998
## BUI              -3.662e+01  4.838e+04  -0.001    0.999
## FWI              -1.914e+01  1.254e+05   0.000    1.000
## Date              3.044e-01  3.141e+02   0.001    0.999
## RegionNew Hudson -5.618e+01  2.111e+04  -0.003    0.998
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.3562e+02  on 171  degrees of freedom
## Residual deviance: 1.5045e-07  on 159  degrees of freedom
## AIC: 26
## 
## Number of Fisher Scoring iterations: 25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41        1
##   not fire    0       30
##                                          
##                Accuracy : 0.9861         
##                  95% CI : (0.925, 0.9996)
##     No Information Rate : 0.5694         
##     P-Value [Acc > NIR] : <2e-16         
##                                          
##                   Kappa : 0.9716         
##                                          
##  Mcnemar's Test P-Value : 1              
##                                          
##             Sensitivity : 1.0000         
##             Specificity : 0.9677         
##          Pos Pred Value : 0.9762         
##          Neg Pred Value : 1.0000         
##              Prevalence : 0.5694         
##          Detection Rate : 0.5694         
##    Detection Prevalence : 0.5833         
##       Balanced Accuracy : 0.9839         
##                                          
##        'Positive' Class : fire           
## 
\end{verbatim}

We immediately note that the p-values for all the predictors are
approximately 1. This is surprising but explainable. As mentioned before
the level of correlation between features is quite high. A tool we can
use to get a better picture of this is VIF.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vif}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Temperature          RH          Ws        Rain        FFMC         DMC 
##    15.28157    19.77930    26.80220   626.12817   876.33536   443.61095 
##          DC         ISI         BUI         FWI        Date      Region 
##   103.42796    83.02431   238.13187   206.50051    21.85894    17.30604
\end{verbatim}

If we consider a high VIF threshold to be 5, we note that all of the
predictors are well over it. Therefore, it is no surprise that variance
and p-value have skyrocketed.

However, this does not mean that this set of data will necessarily
perform badly when predicting the classes. In fact, we note the exact
same results as when we took ISI as a lone predictor. Therefore there is
no need to further analyse or generate a ROC curve. It is only worth
mentioning that because the result where all variables are involved does
not change add much, it indicates that involving all variables is only
slowing down the process.

\paragraph{Best Subset}\label{best-subset}

The logistic regression is now applied to the subset generated from the
best subset selection approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ RH }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ FWI, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ RH + FFMC + DMC + FWI, family = binomial, 
##     data = trainData)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept)   -4.025      6.590  -0.611   0.5414  
## RH             1.989      1.865   1.066   0.2863  
## FFMC         -31.302     24.210  -1.293   0.1960  
## DMC            9.731      5.802   1.677   0.0935 .
## FWI          -24.711     13.651  -1.810   0.0703 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 235.621  on 171  degrees of freedom
## Residual deviance:  11.022  on 167  degrees of freedom
## AIC: 21.022
## 
## Number of Fisher Scoring iterations: 13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41        2
##   not fire    0       29
##                                           
##                Accuracy : 0.9722          
##                  95% CI : (0.9032, 0.9966)
##     No Information Rate : 0.5694          
##     P-Value [Acc > NIR] : 3.744e-15       
##                                           
##                   Kappa : 0.9429          
##                                           
##  Mcnemar's Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.9355          
##          Pos Pred Value : 0.9535          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5694          
##          Detection Rate : 0.5694          
##    Detection Prevalence : 0.5972          
##       Balanced Accuracy : 0.9677          
##                                           
##        'Positive' Class : fire            
## 
\end{verbatim}

We once again note very similar results with this model having predicted
only one extra false positive. It is worth mentioning that none of the
p-values are under 0.05 anymore. The case is not as sever as in the
complete model. Additionally, DMC and FWI are not all that far off and
may even be considered significant depending on how far one is willing
to stretch.

\paragraph{PCA Subset}\label{pca-subset}

Logistic regression is now run on the subset generated by the PCA
approach

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FWI }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ BUI + FWI + DMC + DC, family = binomial, 
##     data = trainData)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept) -14.4259     4.7298  -3.050  0.00229 **
## BUI           1.1640    14.9256   0.078  0.93784   
## FWI         -32.5181    10.5246  -3.090  0.00200 **
## DMC           7.0425    10.9341   0.644  0.51952   
## DC            0.8852     4.6345   0.191  0.84852   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 235.621  on 171  degrees of freedom
## Residual deviance:  17.183  on 167  degrees of freedom
## AIC: 27.183
## 
## Number of Fisher Scoring iterations: 11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41        2
##   not fire    0       29
##                                           
##                Accuracy : 0.9722          
##                  95% CI : (0.9032, 0.9966)
##     No Information Rate : 0.5694          
##     P-Value [Acc > NIR] : 3.744e-15       
##                                           
##                   Kappa : 0.9429          
##                                           
##  Mcnemar's Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.9355          
##          Pos Pred Value : 0.9535          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5694          
##          Detection Rate : 0.5694          
##    Detection Prevalence : 0.5972          
##       Balanced Accuracy : 0.9677          
##                                           
##        'Positive' Class : fire            
## 
\end{verbatim}

Once again we see extremely similar results.

\paragraph{RFE Subset}\label{rfe-subset}

The logistic regression is now run the subset generated by the RFE
method.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fitting the model}
\NormalTok{glm.fits }\OtherTok{=} \FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm.fits)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Classes ~ ISI + FFMC, family = binomial, data = trainData)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)
## (Intercept)   -8.185      7.681  -1.066    0.287
## ISI          -21.222     13.326  -1.593    0.111
## FFMC         -10.893     10.285  -1.059    0.290
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 235.62  on 171  degrees of freedom
## Residual deviance:  15.60  on 169  degrees of freedom
## AIC: 21.6
## 
## Number of Fisher Scoring iterations: 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#creating the array of probabilities}
\NormalTok{glm.probs }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, testData, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.pred }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"not fire"}\NormalTok{,}\DecValTok{72}\NormalTok{)}
\NormalTok{glm.pred[glm.probs }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{5}\NormalTok{] }\OtherTok{=} \StringTok{"fire"}

\CommentTok{\#developing the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(glm.pred), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       41        0
##   not fire    0       31
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9501, 1)
##     No Information Rate : 0.5694     
##     P-Value [Acc > NIR] : < 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar's Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.5694     
##          Detection Rate : 0.5694     
##    Detection Prevalence : 0.5694     
##       Balanced Accuracy : 1.0000     
##                                      
##        'Positive' Class : fire       
## 
\end{verbatim}

Very interestingly, this subset has produced a perfect prediction. There
is perfect accuracy, precision, recall, sensitivity and specificity. As
such, the AUC of this model will certainly be 1 as well.

To get a better idea of what is going on, we can plot the decision
boundary produced by this fit. We begin by defining a range with the
minimum and maximum values of ISI and FFMC. We then create sequences of
equally spaced values between the minimum and maximum values of ISI and
FFMC and put them into a grid. We are defining this set of points in
order to add contour lines that represent our decision boundary. These
lines represent decision boundaries where the predicted probability is
constant.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#create grid for predictions}
\NormalTok{x\_range }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(uns\_trainData}\SpecialCharTok{$}\NormalTok{ISI)}
\NormalTok{y\_range }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(uns\_trainData}\SpecialCharTok{$}\NormalTok{FFMC)}
\NormalTok{x\_vals }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(x\_range[}\DecValTok{1}\NormalTok{], x\_range[}\DecValTok{2}\NormalTok{], }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{y\_vals }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(y\_range[}\DecValTok{1}\NormalTok{], y\_range[}\DecValTok{2}\NormalTok{], }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{ISI =}\NormalTok{ x\_vals, }\AttributeTok{FFMC =}\NormalTok{ y\_vals)}

\CommentTok{\# Add prediction to the grid}
\NormalTok{grid}\SpecialCharTok{$}\NormalTok{predictions }\OtherTok{=} \FunctionTok{predict}\NormalTok{(glm.fits, grid, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# Plot decision boundaries}
\FunctionTok{ggplot}\NormalTok{(uns\_testData, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ISI, }\AttributeTok{y =}\NormalTok{ FFMC, }\AttributeTok{color =}\NormalTok{ Classes)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_contour}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{z =}\NormalTok{ predictions), }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Decision Boundaries with Logistic Regression"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"ISI"}\NormalTok{, }\AttributeTok{y =} \StringTok{"FFMC"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Class"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `stat_contour()`: Zero contours were generated
\end{verbatim}

\begin{verbatim}
## Warning in min(x): no non-missing arguments to min; returning Inf
\end{verbatim}

\begin{verbatim}
## Warning in max(x): no non-missing arguments to max; returning -Inf
\end{verbatim}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-51-1.pdf}

We note that there is a very clear linear separation between the two
classes. This explains why logistic regression produced very optimal
results.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve}

For the remainder of this project, after delving into each model and its
performance, a multiple ROC curve will be generated. This plot will
showcase five ROC curves.

\begin{itemize}
\item
  ROC1: Temperature alone
\item
  ROC2: All predictors
\item
  ROC3: Best Subset
\item
  ROC4: PCA Subset
\item
  ROC5: RFE Subset
\end{itemize}

These five choices will stay consistent throughout in order to be able
to compare performance by the end of the project.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{glm.fit1 }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial))}
\NormalTok{glm.fit2 }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial))}
\NormalTok{glm.fit3 }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FWI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ RH, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial))}
\NormalTok{glm.fit4 }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial))}
\NormalTok{glm.fit5 }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{glm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{family =}\NormalTok{ binomial))}

\CommentTok{\#predict the values}
\NormalTok{glm.prob1}\OtherTok{=}\FunctionTok{predict}\NormalTok{(glm.fit1, testX, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.prob2}\OtherTok{=}\FunctionTok{predict}\NormalTok{(glm.fit2, testX, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.prob3}\OtherTok{=}\FunctionTok{predict}\NormalTok{(glm.fit3, testX, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.prob4}\OtherTok{=}\FunctionTok{predict}\NormalTok{(glm.fit4, testX, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm.prob5}\OtherTok{=}\FunctionTok{predict}\NormalTok{(glm.fit5, testX, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.prob1))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.prob2))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.prob3))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.prob4))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, glm.prob5))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"Logistic Regression"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Logistic Regression ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lr\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\subsubsection{Overall Analysis}\label{overall-analysis}

The logistic regression models performed immensely well on the data
provided. This makes a lot of sense as visually, the Classes look very
separable. Additionally, many of the variables just about follow a
normal distribution. It is no surprise therefore that this type of model
performed so well.

The subset that performed the best was the one generated by RFE.
However, the difference in performance between this subset and the
others is not extremely dire. Aside from the single variable cases where
insignificant predictors were chosen, all models here performed with
great accuracy.

It is worth mentioning that a better idea of the true performance of
these models and the different predictors may be obtained using a larger
dataset. A larger dataset would imply a more extensive and thorough
representation of the true population and might present intricacies that
this model would find hard to deal with. On the flip side, it may just
be that the nature of this data is practically linear. This makes sense
when considering predictors like FWI where higher indices imply more
fires and lower ones imply the opposite.

Overall, based on metrics such as accuracy, precision, recall, AUC, etc.
and over the dataset provided, The logistic regression models were
highly effective.

\subsection{Linear Discriminant Analysis
(LDA)}\label{linear-discriminant-analysis-lda}

We will now fit an LDA model onto our training set. We will begin by
observing Temperature as a feature alone.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.fit}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(Classes ~ Temperature, data = trainData)
## 
## Prior probabilities of groups:
##      fire  not fire 
## 0.5639535 0.4360465 
## 
## Group means:
##          Temperature
## fire       0.5058496
## not fire  -0.5500506
## 
## Coefficients of linear discriminants:
##                  LD1
## Temperature 1.162977
\end{verbatim}

This summary indicates that 56.4\% of entries were chosen to correspond
with the ``fire'' class and 43.6\% were chosen to correspond to the
``not fire'' class. Additionally by observing the means, it becomes
clear that a higher temperature is attributed with ``fire''. Because of
standardization, it becomes unclear as to what this mean temperature
exactly is. If the exact temperature were of interest then either the
standardization must be reversed or the model run over the raw data.

We can plot the linear discriminants to observe the distribution
visually.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(lda.fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-54-1.pdf}

We note that there is a tendency for observations with higher means to
be considered ``fire'' values.

We can next compute the confusion matrix to get a better idea of the
performance of this model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#predict based on test data}
\NormalTok{lda.pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit, testData)}

\CommentTok{\#develop the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(lda.pred}\SpecialCharTok{$}\NormalTok{class), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       29       11
##   not fire   12       20
##                                           
##                Accuracy : 0.6806          
##                  95% CI : (0.5601, 0.7856)
##     No Information Rate : 0.5694          
##     P-Value [Acc > NIR] : 0.03574         
##                                           
##                   Kappa : 0.3511          
##                                           
##  Mcnemar's Test P-Value : 1.00000         
##                                           
##             Sensitivity : 0.7073          
##             Specificity : 0.6452          
##          Pos Pred Value : 0.7250          
##          Neg Pred Value : 0.6250          
##              Prevalence : 0.5694          
##          Detection Rate : 0.4028          
##    Detection Prevalence : 0.5556          
##       Balanced Accuracy : 0.6762          
##                                           
##        'Positive' Class : fire            
## 
\end{verbatim}

We see that this model provides the following metrics:

\begin{itemize}
\item
  Accuracy: 68\%
\item
  Recall: 71\%
\item
  Precision: 72.5\%
\item
  F1 Score: 0.716
\end{itemize}

Overall this is not extremely bad but there is a lot of room for
improvement.

The ROC curve and corresponding AUC can be shown and computed as follows

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-56-1.pdf}

An AUC of 0.795 is not terrible. But once again, there is room for
improvement.

Previously, in Logistic Regression, Wind Speed was observed to be a
lousy predictor when fit by itself. This is because the Classes are not
linearly separable in relation to this feature. However, this does not
necessarily mean that Wind Speed should not be taken into account as a
predictor at all. It is possible that there is a more intricate and less
visual relationship at play that a more flexible model might be better
at grasping. For this reason, it might be interesting to take a look at
this predictor as the complexity of the models used varies. The detailed
metrics will not be delved into and only a ROC curve will be plotted
with an AUC to be used as a comparison metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.fit}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Ws,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit, testData)}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-57-1.pdf}

An AUC of 0.546 is computed. There is no increase in AUC from logistic
regression. This is expected as we are still working on the linear
scale.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-1}

Once again, it would be tiresome and arduous to show detailed metrics
for every choice of subset. Therefore, AUC will be used as a general
metric to gather an idea of the performance of each model. The following
graph shows the ROC curves corresponding to each of the subsets chosen
in the logistic regression model section.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{lda.fit1}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.fit2}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.fit3}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{RH}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{FWI,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.fit4}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{BUI}\SpecialCharTok{+}\NormalTok{FWI}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{DC,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{lda.fit5}\OtherTok{=}\FunctionTok{lda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{ISI}\SpecialCharTok{+}\NormalTok{FFMC,}\AttributeTok{data=}\NormalTok{trainData)}

\CommentTok{\#predict the values}
\NormalTok{lda.pred1}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit1, testData)}
\NormalTok{lda.pred2}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit2, testData)}
\NormalTok{lda.pred3}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit3, testData)}
\NormalTok{lda.pred4}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit4, testData)}
\NormalTok{lda.pred5}\OtherTok{=}\FunctionTok{predict}\NormalTok{(lda.fit5, testData)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred1}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred2}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred3}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred4}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, lda.pred5}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"LDA"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"LDA ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-58-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Once again, the subset generated by RFE produces the best AUC. The other
choices trail closely behind.

There is one interesting point to bring up. Most of the subset selection
methods provided an AUC of 1. However with the slight increase in
complexity of the model used, the AUCs have decreased slightly. This
could be an indication of overfitting as the variance and flexibility of
the model very slightly increases.

\subsection{Quadratic Discriminant Analysis
(QDA)}\label{quadratic-discriminant-analysis-qda}

We are now interested in modelling the data with QDA. From previous
analysis, we note that the nature of the separation between classes is
linear so we are inclined to imagine that this model may perform
slightly worse than the previous two. However, this is not necessarily
guaranteed. Additionally, it might be interesting to see how a nonlinear
model might perform on subsets that were previously thought to be bad.
We will begin by analyzing the fit on a subset containing Temperature
alone.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the model}
\NormalTok{qda.fit}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Classes ~ Temperature, data = trainData)
## 
## Prior probabilities of groups:
##      fire  not fire 
## 0.5639535 0.4360465 
## 
## Group means:
##          Temperature
## fire       0.5058496
## not fire  -0.5500506
\end{verbatim}

Once again, similar to LDA, 56\% of points were chosen to correspond
with ``fire'' entries leaving roughly 44\% corresponding to the
opposite. The trend of higher temperature implying a greater chance of
fire is present in this model as well. It is hard to see if QDA has
improved anything for this choice of subset

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#predict based on test data}
\NormalTok{qda.pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit, testData)}

\CommentTok{\#develop the confusion matrix}
\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data=}\FunctionTok{as.factor}\NormalTok{(qda.pred}\SpecialCharTok{$}\NormalTok{class), }\AttributeTok{reference =}\NormalTok{ testData}\SpecialCharTok{$}\NormalTok{Classes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction fire not fire
##   fire       29       11
##   not fire   12       20
##                                           
##                Accuracy : 0.6806          
##                  95% CI : (0.5601, 0.7856)
##     No Information Rate : 0.5694          
##     P-Value [Acc > NIR] : 0.03574         
##                                           
##                   Kappa : 0.3511          
##                                           
##  Mcnemar's Test P-Value : 1.00000         
##                                           
##             Sensitivity : 0.7073          
##             Specificity : 0.6452          
##          Pos Pred Value : 0.7250          
##          Neg Pred Value : 0.6250          
##              Prevalence : 0.5694          
##          Detection Rate : 0.4028          
##    Detection Prevalence : 0.5556          
##       Balanced Accuracy : 0.6762          
##                                           
##        'Positive' Class : fire            
## 
\end{verbatim}

We note an identical confusion matrix to that of the one generated in
LDA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-61-1.pdf}

This is paired with an identical AUC. Clearly QDA has not improved
performance in the case of Temperature. However, it would still be
interesting to see if it might improve it for variables that were not so
linearly separable. For example, we can try again for Wind Speed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda.fit}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Ws,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit, testData)}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-62-1.pdf}

In fact, there is an increase from 0.546 to 0.619 (13\%) in AUC. Wind
Speed has by no means become a good predictor, but it is interesting to
see the quadratic model work with this variable better than the linear
ones. This implies that Wind Speed may exhibit nonlinear patterns.
Additionally, it reinstates hope that QDA might unearth intricacies that
could improve the fit when considering larger subsets.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{qda.fit1}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.fit2}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.fit3}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{RH}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{FWI,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.fit4}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{BUI}\SpecialCharTok{+}\NormalTok{FWI}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{DC,}\AttributeTok{data=}\NormalTok{trainData)}
\NormalTok{qda.fit5}\OtherTok{=}\FunctionTok{qda}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{ISI}\SpecialCharTok{+}\NormalTok{FFMC,}\AttributeTok{data=}\NormalTok{trainData)}

\CommentTok{\#predict the values}
\NormalTok{qda.pred1}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit1, testData)}
\NormalTok{qda.pred2}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit2, testData)}
\NormalTok{qda.pred3}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit3, testData)}
\NormalTok{qda.pred4}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit4, testData)}
\NormalTok{qda.pred5}\OtherTok{=}\FunctionTok{predict}\NormalTok{(qda.fit5, testData)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred1}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred2}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred3}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred4}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testData}\SpecialCharTok{$}\NormalTok{Classes, qda.pred5}\SpecialCharTok{$}\NormalTok{posterior[,}\DecValTok{1}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"QDA"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"QDA ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-63-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The AUC for the subset containing all features is improved slightly.
This can be attributed to the fact that features exhibiting nonlinear
patterns now have slightly better representation. However, it must still
be inferred that the true nature of the model is that of a linearly
separable one as all of the AUCs are still very high and RFE is still
producing an AUC of 1.

\subsection{K-Nearest Neighbors (KNN)}\label{k-nearest-neighbors-knn}

We will now examine the performance of KNN models on the dataset. This
model assumes no shape when making predictions. It is the most nonlinear
of the models shown up till this point. Seeing as the results have
already been practically optimal in the very simplistic models, it is
unclear if KNN will perform well or if it is even the right choice for
this dataset.

\subsubsection{Training and Evaluation}\label{training-and-evaluation}

Firstly, the KNN method will turn the labels in Classes into variables
at a certain step during its fit. Therefore it is necessary to convert
the labels in order to be compatible; ``not fire'' becomes ``not.fire''.
We will opt to use the unstandardized data to maintain the original
distances between entries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(uns\_trainData}\SpecialCharTok{$}\NormalTok{Classes) }\OtherTok{=} \FunctionTok{make.names}\NormalTok{(}\FunctionTok{levels}\NormalTok{(uns\_trainData}\SpecialCharTok{$}\NormalTok{Classes))}
\end{Highlighting}
\end{Shaded}

The caret package is used to fit the KNN model. As such a 10-fold cross
validation approach is taken. Additionally, a parameter called
tuneLength is specified. It is the number of different k values to try
when fitting the model. This will allow us to first locate the optimal
value of k, and secondly plot the Accuracy of the model at each step to
determine a trend.

We will begin by fitting the model for the subset of all predictors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the model}
\NormalTok{knn.fit }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}

\CommentTok{\#output key information from the fit}
\FunctionTok{head}\NormalTok{(knn.fit}\SpecialCharTok{$}\NormalTok{results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    k  Accuracy     Kappa AccuracySD   KappaSD
## 1  5 0.8897876 0.7730638 0.05741791 0.1190262
## 2  7 0.8776552 0.7479482 0.07087500 0.1474766
## 3  9 0.8776552 0.7469127 0.08999448 0.1894338
## 4 11 0.8828840 0.7571409 0.09629054 0.2019689
## 5 13 0.8828840 0.7590520 0.08794316 0.1811400
## 6 15 0.8773284 0.7479409 0.08122047 0.1676214
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn.fit}\SpecialCharTok{$}\NormalTok{bestTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     k
## 12 27
\end{verbatim}

The optimal values of k was determined to be 27. This is relatively high
and enforces the idea that inflexibility suits the dataset better. We
can plot the results for all the values of k as follows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(knn.fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-67-1.pdf}

We see a general decrease in accuracy as N increase. We note however
that the highest accuracy of 89.4\% comes from the case where k = 27.
Keeping in mind the size of the dataset, this implies that a more
flexible KNN model was opted for. As k keeps increasing, accuracy falls
drastically. However as it approaches 100, accuracy starts rising again.
This may be explainable by the intricacies within the data itself.
However, since when considering the datset in its entirety, we are
working with high dimentionality, it becomes difficult to determine
exactly what is happening.

The ROC curve can be plotted. knn.fit automatically takes the value of k
which generated the highest probability. So it is assumed that k = 27
when generating the following graph.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculate probabilities}
\NormalTok{knn.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}

\CommentTok{\#plot ROC curve}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob[,}\DecValTok{2}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-68-1.pdf}

The AUC of this model when applied to the entirety of the dataset is
0.919. This is currently the lowest recorded AUC for this subset. This
positively reinforces the idea that the dataset is not really suited for
highly nonlinear models.

It would still be interesting to see if KNN could handle Wind Speed
better than the previous models. The ROC curve is generated.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the model}
\NormalTok{knn.fit }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Ws, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}

\CommentTok{\#calculate probabilities}
\NormalTok{knn.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}

\CommentTok{\#plot ROC curve}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob[,}\DecValTok{2}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-69-1.pdf}

The AUC for this ROC curve is actually lower than that of the one
generated in QDA. This is an indication that KNN is simply too nonlinear
to handle the data involved. While QDA may have improved the use of the
predictor slightly, KNN was not able to discern an effective pattern.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{knn.fit1 }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{knn.fit2 }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{knn.fit3 }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ RH }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ FWI, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{knn.fit4 }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{knn.fit5 }\OtherTok{=} \FunctionTok{train}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method=}\StringTok{"knn"}\NormalTok{, }\AttributeTok{tuneLength =} \DecValTok{50}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method=}\StringTok{"cv"}\NormalTok{, }\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{))}

\CommentTok{\#predict the values}
\NormalTok{knn.prob1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit1,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}
\NormalTok{knn.prob2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit2,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}
\NormalTok{knn.prob3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit3,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}
\NormalTok{knn.prob4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit4,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}
\NormalTok{knn.prob5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(knn.fit5,uns\_testData,}\AttributeTok{type=}\StringTok{"prob"}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob1[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob2[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob3[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob4[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testData}\SpecialCharTok{$}\NormalTok{Classes, knn.prob5[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"KNN"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"KNN ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-70-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We see an overall decrease in all the AUCs except for temperature. The
dataset is definitely better off with less flexible approaches.

\subsubsection{K Parameter Analysis}\label{k-parameter-analysis}

The accuracy of each model for different values of k can be plotted and
analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#assign labels to each fit}
\NormalTok{knn.fit1}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"Temperature"}
\NormalTok{knn.fit2}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"All Features"}
\NormalTok{knn.fit3}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"Best Subset"}
\NormalTok{knn.fit4}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"PCA"}
\NormalTok{knn.fit5}\SpecialCharTok{$}\NormalTok{results}\SpecialCharTok{$}\NormalTok{Group }\OtherTok{\textless{}{-}} \StringTok{"RFE"}

\CommentTok{\#combine the results into a single data frame}
\NormalTok{combined\_results }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(knn.fit1}\SpecialCharTok{$}\NormalTok{results, knn.fit2}\SpecialCharTok{$}\NormalTok{results, knn.fit3}\SpecialCharTok{$}\NormalTok{results, knn.fit4}\SpecialCharTok{$}\NormalTok{results, knn.fit5}\SpecialCharTok{$}\NormalTok{results)}

\CommentTok{\#plot the fits}
\NormalTok{K }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(combined\_results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ k, }\AttributeTok{y =}\NormalTok{ Accuracy, }\AttributeTok{colour =}\NormalTok{ Group)) }\SpecialCharTok{+}
      \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"Temperature"} \OtherTok{=} \StringTok{"blue"}\NormalTok{, }\StringTok{"All Features"} \OtherTok{=} \StringTok{"red"}\NormalTok{, }\StringTok{"Best Subset"} \OtherTok{=} \StringTok{"green"}\NormalTok{, }\StringTok{"PCA"} \OtherTok{=} \StringTok{"orange"}\NormalTok{, }\StringTok{"RFE"} \OtherTok{=} \StringTok{"yellow"}\NormalTok{)) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Accuracy for Different K Values"}\NormalTok{, }\AttributeTok{subtitle =} \StringTok{"for the different KNN fits"}\NormalTok{,}\AttributeTok{colour =} \StringTok{"Fit Group"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{theme\_minimal}\NormalTok{()}

\FunctionTok{plot}\NormalTok{(K)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-71-1.pdf}

There is an overall decrease in accuracy as k increase. However, the
peak value of k for each of these fits is relatively high when taking
into consideration the total number of entries in the training set.

\subsection{Tree Methods}\label{tree-methods}

\subsubsection{Reverting Back to Unstandardized
Data}\label{reverting-back-to-unstandardized-data}

The logic behind this step is that tree models are especially good at
being interpretable. With standardized data we lose this interpretation
of the coefficients and split points of the tree. For this reason, the
dataset will be taken in its unstandardized form.

\subsubsection{Simple Classification
Tree}\label{simple-classification-tree}

Using the rpart library, a simple classification tree can be generated.
This will be done taking all the predictors into account at first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{classifier }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\FunctionTok{rpart.plot}\NormalTok{(classifier)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-72-1.pdf}

It seems that the algorithm has determined that the optimal tree depth
is 1. The tree differentiates between the two classes based on FFMC
only. If an entry has an FFMC greater than or equal to 80, it should be
classified as a fire. Any less and the opposite should be predicted. We
already know that FFMC is a strong predictor as it shows up in all three
of the subsets generated by best subset selection, PCA and RFE. Based on
this decision tree, FFMC is the most and only significant predictor.

Just to make sure that we have our classifier running properly, we fit
the model to a subset comprising of Temperature paired with BUI.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{classifier1 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature }\SpecialCharTok{+}\NormalTok{ BUI,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\FunctionTok{rpart.plot}\NormalTok{(classifier1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-73-1.pdf}

This combination generates a more intricate plot. However, at this
point, we are fairly confident that the two predictors chosen will not
offer the best overall performance. Therefore, we stick to our original
subsets and explain the simplistic phenomenon as a result of the
separability of classes. We know from previous analysis and modelling
that the data is highly linearly separable. Therefore a tree with a
depth of 1 may have no trouble picking up on this distinction.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#predicting the test set results}
\NormalTok{y\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier, }
                 \AttributeTok{newdata =}\NormalTok{ uns\_testData, }
                 \AttributeTok{type =} \StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{)}

\CommentTok{\#generate a confusion matrix}
\FunctionTok{table}\NormalTok{(uns\_testY,y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           y_pred
## uns_testY  fire not.fire
##   fire       41        0
##   not fire    2       29
\end{verbatim}

Without needing to calculate, accuracy, precision and recall are all
fairly high. However, the predictor is not perfect and still generates
two false positives

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#predicting the test set to be outputted as probabilities rather than classes}
\NormalTok{y\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier, }
                 \AttributeTok{newdata =}\NormalTok{ uns\_testData, }
                 \AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}

\CommentTok{\#plotting the ROC curve}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred[,}\DecValTok{2}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-75-1.pdf}

The AUC is relatively high reaffirming the accuracy of the simplistic
decision tree.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{classifier1 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\NormalTok{classifier2 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\NormalTok{classifier3 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{FWI }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ RH }\SpecialCharTok{+}\NormalTok{ FFMC,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\NormalTok{classifier4 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{BUI}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{DC,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}
\NormalTok{classifier5 }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{ISI}\SpecialCharTok{+}\NormalTok{FFMC,}\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{method =} \StringTok{"class"}\NormalTok{)}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier1, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier2, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier3, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier4, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(classifier5, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred1[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred2[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred3[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred4[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred5[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"Decision Tree"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Decision Tree ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-76-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

All the AUCs and ROC curves for the subsets excluding Temperature are
the same. This implies that the decision trees for each of these fits
are the same and the same classification was made based on FFMC. When
FFMC is omitted, the model is forced to make a different assumption
resulting in a lower AUC.

\subsubsection{Pruning}\label{pruning}

Obviously since the model has determined a tree of depth = 1 to be
optimal, there is no need for pruning let alone room for it.

\subsubsection{Bagging}\label{bagging}

We begin by fitting the data. 50 bags will be chosen i.e 50 models will
be trained. out-of-bag error is included meaning that excluded entries
from each run are used to estimate the model's performance similar to
cross validation. The minimum number of observations that must exist in
a node for a split to be attempted is set to 2. Finally, cp is set to 0
meaning that the algorithm will attempt to grow the tree to its maximum
size and then prune it.

There is little need to tune these hyperparameters. This was done
anyways in testing and showed that increasing the number of bags or
playing with the complexity didn't change the output. This may be
explained by the simplistic nature of the data and the lack of need for
decision tree splits.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
  
\CommentTok{\#fit the bagged model }
\NormalTok{bag }\OtherTok{=} \FunctionTok{bagging}\NormalTok{( }
  \AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =}\NormalTok{ uns\_trainData, }
  \AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }
  \AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }
  \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{,  }
                         \AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{) }
\NormalTok{)}

\CommentTok{\#predicting the test set results}
\NormalTok{y\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag, uns\_testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{)}

\CommentTok{\#generate a confusion matrix}
\FunctionTok{table}\NormalTok{(uns\_testY,y\_pred}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           
## uns_testY  fire not.fire
##   fire       41        0
##   not fire    2       29
\end{verbatim}

We see there is no change in the output between the simple decision tree
and the one implementing bagging. However, we must analyze some of the
trees before making a definitive analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OtherTok{=} \FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{rpart.plot}\NormalTok{(bag}\SpecialCharTok{$}\NormalTok{trees[[}\DecValTok{3}\NormalTok{]], }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{box.palette =} \StringTok{"auto"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-78-1.pdf}

The decision tree shown above is generated from the third bag. In fact
many of the bags generate this exact tree. This is the simple decision
tree shown in the beginning of this section. However, some bags offer
different results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressWarnings}\NormalTok{(}\FunctionTok{rpart.plot}\NormalTok{(bag}\SpecialCharTok{$}\NormalTok{trees[[}\DecValTok{5}\NormalTok{]], }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{box.palette =} \StringTok{"auto"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-79-1.pdf}

The decision tree shown above is generated from the 5th bag. We note
that now, 1 extra value is marked as ``not fire'' after making a split
based on ISI. These two predictors are the exact predictors that were
provided by RFE early on in the feature selection section.

The ROC curve is generated as follows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#predicting the test set to be outputted as probabilities rather than classes}
\NormalTok{y\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag, }
\NormalTok{                 uns\_testX, }
                 \AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\CommentTok{\#plotting the ROC curve}
\NormalTok{roc\_curve }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}
\FunctionTok{plot}\NormalTok{(roc\_curve, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-80-1.pdf}

The AUC has actually improved a little due to the presence of slightly
more intricate trees.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-5}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{bag1 }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }\AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{))}
\NormalTok{bag2 }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }\AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{))}
\NormalTok{bag3 }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FWI }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ RH }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }\AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{))}
\NormalTok{bag4 }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{DC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }\AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{))}
\NormalTok{bag5 }\OtherTok{=} \FunctionTok{bagging}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI}\SpecialCharTok{+}\NormalTok{FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{nbagg =} \DecValTok{50}\NormalTok{,    }\AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{min\_depth=}\DecValTok{2}\NormalTok{))}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag1, testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag2, testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag3, testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag4, testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(bag5, testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, y\_pred1}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, y\_pred2}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, y\_pred3}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, y\_pred4}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, y\_pred5}\SpecialCharTok{$}\NormalTok{votes[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"Bagged Decision Tree"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Bagged Decision Tree ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-81-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bag\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

There is definitely more variability when considering each of the chosen
subsets. This has resulted in an overall increase in AUC. We note that
the AUC for the RFE subset is equal to 1. This makes sense as the best
decision trees obtained were the ones that combined these two predictors
in the decision making.

\subsubsection{Random Forest}\label{random-forest}

In addition to choosing random assortments of entries, random
assortments of predictors can be chosen as well. In fact this was
already conducted in feature selection. RFE was paired with this method
in order to determine the factors that carried the most weight within
the dataset. Those two factors turned out to be ISI and FFMC. And seeing
as that result is consistent with the results from Bagging, it is
unlikely that there is much more improvement left for Random Forest to
carry out. Regardless, the confusion matrix is generated as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forest }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{) }
\NormalTok{y\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest,uns\_testX) }
\FunctionTok{table}\NormalTok{(uns\_testY, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           y_pred
## uns_testY  fire not.fire
##   fire       40        1
##   not fire    2       29
\end{verbatim}

We see the same confusion matrix as in the other decision tree methods.
To avoid repetitive analysis, we jump straight to the multiple ROC curve

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-6}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{forest1 }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{Temperature, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{)}
\NormalTok{forest2 }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{)}
\NormalTok{forest3 }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{RH}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{FWI, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{)}
\NormalTok{forest4 }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{BUI}\SpecialCharTok{+}\NormalTok{FFMC}\SpecialCharTok{+}\NormalTok{DMC}\SpecialCharTok{+}\NormalTok{DC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{)}
\NormalTok{forest5 }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Classes}\SpecialCharTok{\textasciitilde{}}\NormalTok{ISI}\SpecialCharTok{+}\NormalTok{FFMC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{)}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest1, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest2, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest3, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest4, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(forest5, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred1[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred2[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred3[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred4[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred5[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"Random Forest"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Random Forest ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-83-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

There is a slight decrease in the AUC of the RFE fit. This makes sense
as random forest was taking subsets of these two predictors rather than
allowing them to completely coexist. However, all the AUCs are generally
high.

\subsubsection{Boosting}\label{boosting}

The final method of creating decision trees is Boosting. Here, trees are
developed sequentially with weight being added to elements that are
missclassified each step. The parameter mfinal denotes the number of
trees to make. 100 is chosen and should be sufficient for the size of
the data at hand. (In private testing, increasing this parameter did not
change the results). A cp of 0 is chosen allowing for trees to grow as
deep as possible and a minsplit of 2 allows for groups to contain single
elements.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{model }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \DecValTok{0}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}
 
\CommentTok{\# Make predictions on the test set}
\NormalTok{predictions }\OtherTok{=} \FunctionTok{predict}\NormalTok{(model, uns\_testX)}
 
\CommentTok{\# Calculate the confusion matrix}
\FunctionTok{table}\NormalTok{(uns\_testY,predictions}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           
## uns_testY  fire not.fire
##   fire       41        0
##   not fire    2       29
\end{verbatim}

The confusion matrix is once again the same as in all the other cases.

\subsubsection{Multiple ROC Curve}\label{multiple-roc-curve-7}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{boost1 }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}
\NormalTok{boost2 }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}
\NormalTok{boost3 }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FWI }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ RH, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}
\NormalTok{boost4 }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}
\NormalTok{boost5 }\OtherTok{=} \FunctionTok{boosting}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{boos =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{mfinal =} \DecValTok{100}\NormalTok{, }\AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{cp =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{))}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(boost1, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(boost2, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(boost3, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(boost4, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(boost5, }\AttributeTok{newdata =}\NormalTok{ uns\_testData, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred1}\SpecialCharTok{$}\NormalTok{prob[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred2}\SpecialCharTok{$}\NormalTok{prob[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred3}\SpecialCharTok{$}\NormalTok{prob[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred4}\SpecialCharTok{$}\NormalTok{prob[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(uns\_testY, y\_pred5}\SpecialCharTok{$}\NormalTok{prob[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"boosted decision tree"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Boosting Decision Tree ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-85-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boost\_plot }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Once again, results are very similar to the other decision tree
algorithms. There is not much room left to improve and these methods
have shown to fit the dataset really well on the whole.

\subsection{Support Vector Machine
(SVM)}\label{support-vector-machine-svm}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(trainData}\SpecialCharTok{$}\NormalTok{Classes) }\OtherTok{=} \FunctionTok{make.names}\NormalTok{(}\FunctionTok{levels}\NormalTok{(trainData}\SpecialCharTok{$}\NormalTok{Classes))}
\end{Highlighting}
\end{Shaded}

The final model to be discussed in the project is SVM. Using all of the
information gathered about the dataset so far, a linear kernel should
have no problem classifying the data. To illustrate this the hyperplane
can be plotted for the SVM between ISI and FFMC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(svmfit, }\FunctionTok{cbind}\NormalTok{(trainData[}\DecValTok{5}\NormalTok{],trainData[}\DecValTok{8}\NormalTok{],trainData[}\DecValTok{11}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-87-1.pdf}

We see that while the method may misclassify one or two data points, it
does a very good job of separating the two classes.

However, it would be unfair to completely write off other kernels.
Additionally, there is also a need to determine the optimal
hyperparameters.

\subsubsection{Linear Kernel}\label{linear-kernel}

Without tuning, the linear kernel outputs the following confusion
matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svm\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit,testX, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{table}\NormalTok{(svm\_pred,testY)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           testY
## svm_pred   fire not fire
##   fire       40        2
##   not.fire    1       29
\end{verbatim}

There is an accuracy of roughly 96\%. And while this alone is not enough
to determine the true predictive power of this specific model, we are
less interested in this than we are in the potential for better results
after hyperparameter tuning.

\paragraph{Hyperparameter Tuning}\label{hyperparameter-tuning}

For a linear Kernel, the primary hyperparameter is C. This is the
parameter that controls the trade-off between low training error and low
testing error. In order to tune it, a range from 0.1 to 100 is tested
and the best cost is outputted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune.fit }\OtherTok{=} \FunctionTok{tune.svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{type =} \StringTok{"C{-}classification"}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\SpecialCharTok{\^{}}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{))}

\NormalTok{tune.fit}\SpecialCharTok{$}\NormalTok{best.model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## best.svm(x = Classes ~ ., data = trainData, cost = 10^(-1:2), type = "C-classification", 
##     kernel = "linear")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  27
\end{verbatim}

According to the above, the best value of C is 1. This is the default
value used. With this knowledge the best achievable results with the
linear kernel are the ones shown above.

Without calculating metrics, the linear kernel has performed well and
classifies with fair accuracy. To the determine the extent however, the
multiple ROC curve is drawn.

\paragraph{Multiple ROC Curve}\label{multiple-roc-curve-8}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{svmfit1 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit2 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit3 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FWI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ RH, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit4 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit5 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\AttributeTok{cost =} \DecValTok{1}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit1, testX, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit2, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit3, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit4, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit5, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred1, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred2, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred3, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred4, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred5, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"SVM (Linear Kernel)"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}


\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"SVM Linear Kernel ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.4}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.3}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.2}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{print.auc =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{print.auc.y =} \FloatTok{0.1}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-90-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_lin }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The AUCs of each of the feature selected subsets are all almost equal to
1. As expected, the linear kernel does a good job of separating the
classes.

\subsubsection{Polynomial Kernel}\label{polynomial-kernel}

Without tuning, the polynomial kernel outputs the following confusion
matrix

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svm\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit,testX)}
\FunctionTok{table}\NormalTok{(svm\_pred,testY)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           testY
## svm_pred   fire not fire
##   fire       41        6
##   not.fire    0       25
\end{verbatim}

With an accuracy of 92\%, this model is currently performing worse than
the one with the linear kernel. In order to see if this can be changed,
hyperparameters must be tuned.

\paragraph{Hyperparameter Tuning}\label{hyperparameter-tuning-1}

In the case of a polynomial kernel, we are interested in a few more
hyperparameters. In addition to C, we are now also curious about gamma.
Gamma is a factor which basically controls the curvature of the decision
boundary. We would also like to determine coef0, another value
influencing the shape of the boundary. We do so in a similar way to the
linear kernel approach. Gamma is given a choice between 0.1, 1 and 10
and the same applies for coef0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune.fit }\OtherTok{=} \FunctionTok{tune.svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ uns\_trainData, }\AttributeTok{type =} \StringTok{"C{-}classification"}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\SpecialCharTok{\^{}}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{), }\AttributeTok{gamma =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{coef0 =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{))}

\NormalTok{tune.fit}\SpecialCharTok{$}\NormalTok{best.parameters}\SpecialCharTok{$}\NormalTok{cost}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune.fit}\SpecialCharTok{$}\NormalTok{best.parameters}\SpecialCharTok{$}\NormalTok{gamma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune.fit}\SpecialCharTok{$}\NormalTok{best.parameters}\SpecialCharTok{$}\NormalTok{coef0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

For the polynomial kernel, a cost of 10, gamma of 0.1 and coef0 of 10 is
preferred.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svm\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit,testX)}
\FunctionTok{table}\NormalTok{(svm\_pred,testY)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           testY
## svm_pred   fire not fire
##   fire       41        1
##   not.fire    0       30
\end{verbatim}

Surprisingly, the polynomial kernel seems to have outperformed the
linear kernel with an accuracy of 98.6\%. The accuracy is not enough on
its own, so the multiple ROC curve is shown.

\paragraph{Multiple ROC Curve}\label{multiple-roc-curve-9}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#fit the models}
\NormalTok{svmfit1 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Temperature, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit2 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit3 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ RH }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ FWI, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit4 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ BUI }\SpecialCharTok{+}\NormalTok{ FFMC }\SpecialCharTok{+}\NormalTok{ DMC }\SpecialCharTok{+}\NormalTok{ DC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{svmfit5 }\OtherTok{=} \FunctionTok{svm}\NormalTok{(Classes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ISI }\SpecialCharTok{+}\NormalTok{ FFMC, }\AttributeTok{data =}\NormalTok{ trainData, }\AttributeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\AttributeTok{gamma =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{, }\AttributeTok{cost =} \DecValTok{10}\NormalTok{, }\AttributeTok{coef0 =} \DecValTok{10}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#predict the values}
\NormalTok{y\_pred1 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit1, testX, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred2 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit2, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred3 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit3, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred4 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit4, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y\_pred5 }\OtherTok{=} \FunctionTok{predict}\NormalTok{(svmfit5, testX, }\AttributeTok{type =} \StringTok{\textquotesingle{}prob\textquotesingle{}}\NormalTok{, }\AttributeTok{probability =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#compute ROC curves}
\NormalTok{roc1 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred1, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc2 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred2, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc3 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred3, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc4 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred4, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}
\NormalTok{roc5 }\OtherTok{=} \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{roc}\NormalTok{(testY, }\FunctionTok{attr}\NormalTok{(y\_pred5, }\StringTok{"probabilities"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]))}

\CommentTok{\#compute aucs}
\NormalTok{auc1 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc1)}
\NormalTok{auc2 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc2)}
\NormalTok{auc3 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc3)}
\NormalTok{auc4 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc4)}
\NormalTok{auc5 }\OtherTok{=} \FunctionTok{auc}\NormalTok{(roc5)}

\CommentTok{\#create a dataframe and store AUC to be used in comparison later}
\NormalTok{auc1.data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Model =} \StringTok{"SVM (Quadratic Kernel)"}\NormalTok{,}
  \AttributeTok{Temperature =}\NormalTok{ auc1,}
  \AttributeTok{Entire\_Dataset =}\NormalTok{ auc2,}
  \AttributeTok{Best\_Subset =}\NormalTok{ auc3,}
  \AttributeTok{PCA =}\NormalTok{ auc4,}
  \AttributeTok{RFE =}\NormalTok{ auc5}
\NormalTok{)}

\NormalTok{auc.data }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(auc.data,auc1.data)}

\CommentTok{\#plot the first ROC curve}
\FunctionTok{plot.roc}\NormalTok{(roc1, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"SVM Polynomial Kernel ROC Curves"}\NormalTok{)}

\CommentTok{\#add the rest of the curves as lines}
\FunctionTok{lines.roc}\NormalTok{(roc2, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc3, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc4, }\AttributeTok{col =} \StringTok{"orange"}\NormalTok{)}

\FunctionTok{lines.roc}\NormalTok{(roc5, }\AttributeTok{col =} \StringTok{"yellow"}\NormalTok{)}

\NormalTok{legend\_labels }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Temperature (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc1, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"All Features (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc2, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Best Subset (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc3, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"PCA (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc4, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"RFE (AUC ="}\NormalTok{, }\FunctionTok{round}\NormalTok{(auc5, }\DecValTok{4}\NormalTok{), }\StringTok{")"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\#legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ legend\_labels, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{), }\AttributeTok{lwd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-94-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_poly }\OtherTok{=} \FunctionTok{recordPlot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

There is actually a slight improvement in AUC across the board using the
polynomial kernel. This could be attributed to the fact that the
hyperparameter tuning allowed for a slightly more optimal decision
boundary to be produced allowing for a better depiction of the
intricacies within the test set.

\section{Results Interpretation}\label{results-interpretation}

The main metric used across all models to obtain a measure of
performance was AUC. Confusion matrices were generated for each model
but due to repetition and similarity, the computation of metrics such as
accuracy, recall and precision were eventually omitted.

The multiple ROC curves generated throughout this project are all
displayed below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(lr\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(lda\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(qda\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(knn\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(tree\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-5.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(bag\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-6.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(rf\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-7.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(boost\_plot)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-8.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(svm\_lin)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-9.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(svm\_poly)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-95-10.pdf}

A few things are visually clear when comparing all of these figures
together:

\begin{itemize}
\item
  The strongest subset was generally the one generated by RFE

  This is due to the nature of the dataset being highly separable /
  linearly separable. Random Forest was used as the method of evaluation
  in RFE. Because Random Forest is ultimately a decision tree method, it
  was able to divide the dataset based on the predictors given in a way
  that separated the classes almost perfectly. Additionally, when
  studying ISI and FFMC together, it is made very clear how separable
  the two are. This made it easy for every model to pick up on the
  separability and hone in on it.
\item
  The maximum AUC for each model is very high

  This is likely due to the simplicity of the model. Once again the
  nature of the dataset is highly simplistic and so any of the given
  models should not have a hard time finding an optimal solution.
  Additionally, there are few data points which probably added to the
  simplicity. This is especially true when you take into account that
  all the values were gathered around summer in the span of three
  months. The set of values most likely present a local view of the true
  nature rather than a global one.
\item
  The model that seems to have performed the worst is KNN

  KNN is a nonlinear method. It can be highly flexible when k is small
  and less so when large. However, due to the size of the dataset,
  taking a value of k around 100 is already pushing it. As k approaches
  the number of elements in the dataset, the grouping becomes dependent
  on which class is larger. Additionally, the nature of the data was
  highly simplistic and did not call for such a flexible/nonlinear
  model.
\end{itemize}

A dataframe containing all of these AUCs can be studied to get a further
picture of these results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auc.data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     Model Temperature Entire_Dataset Best_Subset       PCA
## 1     Logistic Regression   0.7954367      1.0000000   0.9976397 1.0000000
## 2                     LDA   0.7954367      0.9889851   0.9913454 0.9866247
## 3                     QDA   0.7954367      0.9984264   0.9992132 0.9944925
## 4                     KNN   0.7966168      0.8922109   0.9712825 0.9803304
## 5           Decision Tree   0.6809599      0.9677419   0.9677419 0.9677419
## 6    Bagged Decision Tree   0.7454760      0.9992132   0.9956727 0.9984264
## 7           Random Forest   0.6746656      0.9952793   0.9952793 0.9921322
## 8   boosted decision tree   0.7202990      0.9976397   0.9944925 0.9937057
## 9     SVM (Linear Kernel)   0.7954367      0.9960661   0.9968529 0.9960661
## 10 SVM (Quadratic Kernel)   0.7954367      1.0000000   0.9968529 0.9984264
##          RFE
## 1  1.0000000
## 2  1.0000000
## 3  1.0000000
## 4  0.9972463
## 5  0.9677419
## 6  1.0000000
## 7  0.9984264
## 8  0.9964595
## 9  1.0000000
## 10 1.0000000
\end{verbatim}

\begin{itemize}
\item
  In the case where Temperature was used as a lone predictor, KNN
  actually performed the best. It is possible that in this lone case,
  the flexibility of the method was able to pick up on certain
  intricacies. However, seeing as the AUC is not all that much greater
  than some of the other models', this can also likely be attributed to
  a fluke favoring the KNN algorithm.
\item
  When considering the entire dataset, the two models which performed
  the best were Logistic Regression and SVM with a quadratic kernel. It
  is highly understandable why logistic regression might perform so well
  on this dataset, but potentially less so when considering quadratic
  SVM. One possible explanation is that the predictors exhibiting
  nonlinear characteristics were taken advantage of more in the latter
  model and therefore the high AUC was contrived in a different way.
\end{itemize}

The AUCs of each model can be plotted in a bar graph as shown.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auc.data[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(auc.data[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], as.numeric)}

\NormalTok{df\_long }\OtherTok{=} \FunctionTok{pivot\_longer}\NormalTok{(auc.data, }\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{Model, }\AttributeTok{names\_to =} \StringTok{"Feature\_Set"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"AUC"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(df\_long, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Model, }\AttributeTok{y =}\NormalTok{ AUC, }\AttributeTok{fill =}\NormalTok{ Feature\_Set)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Model AUC Across Different Feature Sets"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Model"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"AUC"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Data-Mining-Final-Project_files/figure-latex/unnamed-chunk-97-1.pdf}

One last interesting point of analysis to be made here is by looking at
the tree methods. It is clear that the simple Decision Tree performed
worse than the ensemble methods. While the simple decision tree's AUCs
are not bad at all, the others' are just ever so slightly better. The
nature of the ensemble methods picked up on small factors which the
simpler model was not able to do and therefore showcased better results.

Otherwise, the only other thing that this plot shows is the fact that
every model performed exceedingly well.

\section{Conclusion}\label{conclusion}

Through this project, several machine learning models were used to
predict forest fires. The performance of each of these models was
measured and interpreted based on key metrics such as accuracy, AUC, and
ROC curves. All of the methods used produced strong predictive results.
This was due to many factors including the simplicity of the model, the
linear separability, and limited data points. There was a slight trend
that indicated that more flexible models tended to overfit the data more
and thus perform slightly worse. But even KNN, being one of the most
nonlinear models of the bunch, performed better than well. Aside from
the models, this project also highlights the importance of feature
selection, preprocessing, and model tuning in achieving high predictive
accuracy and gaining a greater understanding of the data at hand. The
results underscore the potential of machine learning in environmental
risk prediction, and offer potential for similar critical studies in
related fields.

\end{document}
